{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process\n",
    "== \n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.basics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column renaming and adding of start and end events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def df_preproc (df,cols=['activity'],start_marker='###start###',end_marker='###end###'):\n",
    "    \"\"\"\n",
    "    Add event_id column, as start and end tokens for each trace\n",
    "    - Copys entire row.\n",
    "    - Adds special start and end marker for col in `cols`\n",
    "    \n",
    "    \"\"\"\n",
    "    # Add event log column\n",
    "    df['event_id']=df.groupby('trace_id').cumcount()+1    \n",
    "    # Work with numpy for performance boost\n",
    "    eid_col = df.columns.to_list().index('event_id')\n",
    "       \n",
    "    # Get col idx\n",
    "    col_idx=[df.columns.to_list().index(c) for c in cols]\n",
    "        \n",
    "    data= df.values\n",
    "    # Add Start Events\n",
    "    idx= np.where(data[:,eid_col]==1)[0] # start idx\n",
    "    new = data[idx].copy()\n",
    "    \n",
    "    for c in col_idx: new[:,c]=start_marker\n",
    "    new[:,eid_col]=0\n",
    "    data = np.insert(data,idx,new, axis=0)\n",
    "    # Add End Events\n",
    "    idx= np.where(data[:,eid_col]==0)[0][1:] # start idx without the first\n",
    "    new = data[idx-1].copy() # get data from current last idx\n",
    "    for c in col_idx: new[:,c]=end_marker\n",
    "    \n",
    "    new[:,eid_col]=new[:,eid_col]+1\n",
    "    data = np.insert(data,idx,new, axis=0)\n",
    "    # take care of final last event\n",
    "    last= data[-1].copy()\n",
    "    for c in col_idx: last[c]=end_marker\n",
    "\n",
    "    last[eid_col]+=1\n",
    "    data = np.insert(data,len(data),last, axis=0)\n",
    "    df = pd.DataFrame(data,columns=df.columns)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports an event log in csv format\n",
    "\n",
    "    - rename activity and trace_id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def import_log(log_path,cols=['activity']):\n",
    "    \"\"\"\n",
    "    Imports an event log in csv format\n",
    "    - rename activity and trace_id column\n",
    "    - rename anomaly column\n",
    "    \"\"\"\n",
    "    df=pd.read_csv(log_path)\n",
    "    df.rename({'name':'activity','case:concept:name':'trace_id'},axis=1,inplace=True)\n",
    "    if not 'activity' in df.columns:\n",
    "        df.rename({'concept:name':'activity'},axis=1,inplace=True)\n",
    "    df = df_preproc(df,cols)\n",
    "    df.index=df.trace_id\n",
    "    df.drop('trace_id',axis=1,inplace=True,)\n",
    "    return df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>resource</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>activity</th>\n",
       "      <th>REG_DATE</th>\n",
       "      <th>AMOUNT_REQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trace_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>###start###</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>1</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>A_SUBMITTED_COMPLETE</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>2</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011-09-30 22:38:44.880000+00:00</td>\n",
       "      <td>A_PARTLYSUBMITTED_COMPLETE</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>3</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011-09-30 22:39:37.906000+00:00</td>\n",
       "      <td>A_PREACCEPTED_COMPLETE</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011-09-30 22:39:38.875000+00:00</td>\n",
       "      <td>W_Completeren aanvraag_SCHEDULE</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         event_id resource                         timestamp  \\\n",
       "trace_id                                                       \n",
       "173688          0    112.0  2011-09-30 22:38:44.546000+00:00   \n",
       "173688          1    112.0  2011-09-30 22:38:44.546000+00:00   \n",
       "173688          2    112.0  2011-09-30 22:38:44.880000+00:00   \n",
       "173688          3    112.0  2011-09-30 22:39:37.906000+00:00   \n",
       "173688          4    112.0  2011-09-30 22:39:38.875000+00:00   \n",
       "\n",
       "                                 activity                          REG_DATE  \\\n",
       "trace_id                                                                      \n",
       "173688                        ###start###  2011-10-01 00:38:44.546000+02:00   \n",
       "173688               A_SUBMITTED_COMPLETE  2011-10-01 00:38:44.546000+02:00   \n",
       "173688         A_PARTLYSUBMITTED_COMPLETE  2011-10-01 00:38:44.546000+02:00   \n",
       "173688             A_PREACCEPTED_COMPLETE  2011-10-01 00:38:44.546000+02:00   \n",
       "173688    W_Completeren aanvraag_SCHEDULE  2011-10-01 00:38:44.546000+02:00   \n",
       "\n",
       "         AMOUNT_REQ  \n",
       "trace_id             \n",
       "173688        20000  \n",
       "173688        20000  \n",
       "173688        20000  \n",
       "173688        20000  \n",
       "173688        20000  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = '../data/logs/csv/mppn_ds/BPIC12.csv'\n",
    "log = import_log(log_path)\n",
    "log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace Splitting\n",
    "i.e. splitting in training, validation and test set\n",
    "\n",
    "The split_traces function is used to split an event_log into training, validation and test set. Furthermore, it removes traces that are longer than a specific threshhold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def drop_long_traces(df,max_trace_len=64,event_id='event_id'):\n",
    "    \"Drop traces longer than `max_trace_len`\"\n",
    "    df=df.drop(np.unique(df[df[event_id]>max_trace_len].index))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def RandomTraceSplitter(split_pct=0.2, seed=None):\n",
    "    \"Create function that splits `items` between train/val with `valid_pct` randomly.\"\n",
    "    def _inner(trace_ids):\n",
    "        o=np.unique(trace_ids)\n",
    "        np.random.seed(seed)\n",
    "        rand_idx = np.random.permutation(o)\n",
    "        cut = int(split_pct * len(o))\n",
    "        return L(rand_idx[cut:].tolist()),L(rand_idx[:cut].tolist())\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_traces(df,df_name='tmp',seed = None):\n",
    "    \"Split traces in training, validation and test sets\"\n",
    "    df=drop_long_traces(df)\n",
    "    ts=RandomTraceSplitter(seed=seed)\n",
    "    train,test=ts(df.index)\n",
    "    ts=RandomTraceSplitter(seed=seed,split_pct=0.1)\n",
    "    train,valid=ts(train)\n",
    "    return train,valid,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,b1,c1=split_traces(log)\n",
    "a2,b2,c2=split_traces(log)\n",
    "test_ne(a1,a2), test_ne(b1,b2), test_ne(c1,c2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "a1,b1,c1=split_traces(log,seed = seed)\n",
    "a2,b2,c2=split_traces(log,seed = seed)\n",
    "test_eq(a1,a2), test_eq(b1,b2), test_eq(c1,c2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPObj\n",
    "Manages the pre-processing and provides utility functiosn for date columns, cat columns and cont columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _TraceIloc:\n",
    "    \"Get/set rows by iloc and cols by name\"\n",
    "    def __init__(self,o): self.o = o\n",
    "    def __getitem__(self, idxs):\n",
    "        df = self.o.items\n",
    "        if isinstance(idxs,tuple):\n",
    "            rows,cols = idxs\n",
    "            rows=df.index[rows]\n",
    "            return self.o.new(df.loc[rows,cols])\n",
    "        else:\n",
    "            rows,cols = idxs,slice(None)\n",
    "            rows=np.unique(df.index)[rows]\n",
    "            return self.o.new(df.loc[rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Not so useful, should be cleaned up!\n",
    "\n",
    "class PPObj(CollBase, GetAttr, FilteredBase):\n",
    "    \"Main Class for Process Prediction\"\n",
    "    _default,with_cont='procs',True\n",
    "    def __init__(self,df,procs=None,cat_names=None,cont_names=None,date_names=None,y_names=None,splits=None,\n",
    "                 ycat_names=None,ycont_names=None,inplace=False,do_setup=True):\n",
    "        if not inplace: df=df.copy()\n",
    "        if splits is not None: df = df.loc[sum(splits, [])] # Can drop traces\n",
    "        self.event_ids=df['event_id'].values if hasattr(df,'event_id') else None\n",
    "\n",
    "        super().__init__(df)\n",
    "\n",
    "        self.cat_names,self.cont_names,self.date_names=(L(cat_names),L(cont_names),L(date_names))\n",
    "        self.set_y_names(y_names,ycat_names,ycont_names)\n",
    "\n",
    "        self.procs = Pipeline(procs)\n",
    "        self.splits=splits\n",
    "        if do_setup: self.setup()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def y_names(self): return self.ycat_names+self.ycont_names\n",
    "\n",
    "    def set_y_names(self,y_names,ycat_names=None,ycont_names=None):\n",
    "        if ycat_names or ycont_names: store_attr('ycat_names,ycont_names')\n",
    "        else:\n",
    "            self.ycat_names,self.ycont_names=(L([i for i in L(y_names) if i in self.cat_names]),\n",
    "                                                L([i for i in L(y_names) if i not in self.cat_names]))\n",
    "    def setup(self): self.procs.setup(self)\n",
    "    def subset(self, i): return self.new(self.loc[self.splits[i]]) if self.splits else self\n",
    "    def __len__(self): return len(np.unique(self.items.index))\n",
    "    def show(self, max_n=3, **kwargs):\n",
    "        print('#traces:',len(self),'#events:',len(self.items))\n",
    "        display_df(self.new(self.all_cols[:max_n]).items)\n",
    "    def new(self, df):\n",
    "        return type(self)(df, do_setup=False,\n",
    "                          **attrdict(self, 'procs','cat_names','cont_names','ycat_names','ycont_names',\n",
    "                                     'date_names'))\n",
    "    def process(self): self.procs(self)\n",
    "    def loc(self): return self.items.loc\n",
    "    def iloc(self): return _TraceIloc(self)\n",
    "    def x_names (self): return self.cat_names + self.cont_names\n",
    "    def all_col_names(self): return ((self.x_names+self.y_names)).unique()\n",
    "    def transform(self, cols, f, all_col=True):\n",
    "        if not all_col: cols = [c for c in cols if c in self.items.columns]\n",
    "        if len(cols) > 0: self[cols] = self[cols].transform(f)\n",
    "    def new_empty(self): return self.new(pd.DataFrame({}, columns=self.items.columns))\n",
    "    def subsets(self): return [self.subset(i) for i in range(len(self.splits))] if self.splits else L(self)\n",
    "properties(PPObj,'loc','iloc','x_names','all_col_names')\n",
    "\n",
    "def _add_prop(cls, nm):\n",
    "    @property\n",
    "    def f(o): return o[list(getattr(o,nm+'_names'))]\n",
    "    @f.setter\n",
    "    def fset(o, v): o[getattr(o,nm+'_names')] = v\n",
    "    setattr(cls, nm+'s', f)\n",
    "    setattr(cls, nm+'s', fset)\n",
    "\n",
    "_add_prop(PPObj, 'cat')\n",
    "_add_prop(PPObj, 'cont')\n",
    "_add_prop(PPObj, 'ycat')\n",
    "_add_prop(PPObj, 'ycont')\n",
    "_add_prop(PPObj, 'y')\n",
    "_add_prop(PPObj, 'x')\n",
    "_add_prop(PPObj, 'all_col')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#traces: 13087 #events: 288374\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trace_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>###start###</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>A_SUBMITTED_COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>A_PARTLYSUBMITTED_COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppObj=PPObj(log,cat_names=['activity'],y_names=['activity'])\n",
    "ppObj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define various pre-processing functions that are executed, when `PPOBj` is instantiated. `PPProc` is the base class for a pre-processing function. It ensures, that setup of a pre-processing function is performed using the training set, and than it is applied to the validation and test set, with the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PPProc(InplaceTransform):\n",
    "    \"Base class to write a non-lazy tabular processor for dataframes\"\n",
    "    def setup(self, items=None, train_setup=False): #TODO: properly deal with train_setup\n",
    "        super().setup(getattr(items,'train',items), train_setup=False)\n",
    "        #super().setup(items, train_setup=False)\n",
    "\n",
    "        # Procs are called as soon as data is available\n",
    "        return self(items.items if isinstance(items,Datasets) else items)\n",
    "\n",
    "    @property\n",
    "    def name(self): return f\"{super().name} -- {getattr(self,'__stored_args__',{})}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorization\n",
    "i.e ordinal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of ordinal or integer encoding. Adds NA values for unknown data. Implementation is pretty much taken from fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_cats (voc, add, c):\n",
    "    if not is_categorical_dtype(c):\n",
    "        return pd.Categorical(c, categories=voc[c.name][add:]).codes+add\n",
    "    return c.cat.codes+add #if is_categorical_dtype(c) else c.map(voc[c.name].o2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Categorify(PPProc):\n",
    "    \"Transform the categorical variables to something similar to `pd.Categorical`\"\n",
    "    order = 2\n",
    "    def setups(self, to):\n",
    "        store_attr(classes={n:CategoryMap(to.items.loc[:,n], add_na=True) for n in to.cat_names}, but='to')\n",
    "    def encodes(self, to):\n",
    "        to.transform(to.cat_names, partial(_apply_cats, self.classes, 1))\n",
    "    def __getitem__(self,k): return self.classes[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=import_log('../data/logs/csv/dapnn_ds/PDC2021_ground_truth/pdc2021_000000.csv.gz')\n",
    "traces=split_traces(log)[0][:100]\n",
    "splits=traces[:60],traces[60:80],traces[80:100]\n",
    "o=PPObj(log,None,cat_names='activity',splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=CategoryMap(o.items.loc[:,'activity'])\n",
    "len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat=Categorify()\n",
    "cat.setup(o)\n",
    "len(cat['activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#traces: 5 #events: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame({'a':[0,1,2,0,2]})\n",
    "to = PPObj(df, Categorify, 'a')\n",
    "to.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#traces: 13087 #events: 289892\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trace_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log=import_log('../data/logs/csv/dapnn_ds/binet_logs/bpic12-0.3-1.csv.gz')\n",
    "o=PPObj(log,Categorify,'activity')\n",
    "o.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Missing\n",
    "for continuous values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pre-processing function that deals with missing data in continuous attributes. Missing data can be replaced with the median, mean or a constant value. Additionaly, we can create another boolean column that indicates, which rows were missing.  Implementation is pretty much taken from fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FillStrategy:\n",
    "    \"Namespace containing the various filling strategies.\"\n",
    "    def median  (c,fill): return c.median()\n",
    "    def constant(c,fill): return fill\n",
    "    def mode    (c,fill): return c.dropna().value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FillMissing(PPProc):\n",
    "    order=1\n",
    "    \"Fill the missing values in continuous columns.\"\n",
    "    def __init__(self, fill_strategy=FillStrategy.median, add_col=True, fill_vals=None):\n",
    "        if fill_vals is None: fill_vals = defaultdict(int)\n",
    "        store_attr()\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        missing = pd.isnull(dsets.conts).any()\n",
    "        store_attr(but='to', na_dict={n:self.fill_strategy(dsets[n], self.fill_vals[n])\n",
    "                            for n in missing[missing].keys()})\n",
    "        self.fill_strategy = self.fill_strategy.__name__\n",
    "\n",
    "    def encodes(self, to):\n",
    "        missing = pd.isnull(to.conts)\n",
    "        for n in missing.any()[missing.any()].keys():\n",
    "            assert n in self.na_dict, f\"nan values in `{n}` but not in setup training set\"\n",
    "        for n in self.na_dict.keys():\n",
    "            to[n].fillna(self.na_dict[n], inplace=True)\n",
    "            if self.add_col:\n",
    "                to.loc[:,n+'_na'] = missing[n]\n",
    "                if n+'_na' not in to.cat_names: to.cat_names.append(n+'_na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#traces: 7 #events: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_na</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fill = FillMissing() \n",
    "df = pd.DataFrame({'a':[0,1,np.nan,1,2,3,4], 'b': [0,1,2,3,4,5,6]})\n",
    "to = PPObj(df, fill, cont_names=['a', 'b'])\n",
    "to.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Normalize(PPProc):\n",
    "    \"Normalize with z-score\"\n",
    "    order = 3\n",
    "    def setups(self, to):\n",
    "        store_attr(but='to', means=dict(getattr(to, 'train', to).conts.mean()),\n",
    "                   stds=dict(getattr(to, 'train', to).conts.std(ddof=0)+1e-7))\n",
    "        return self(to)\n",
    "\n",
    "    def encodes(self, to): to.conts = (to.conts-self.means) / self.stds\n",
    "    def decodes(self, to): to.conts = (to.conts*self.stds ) + self.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#traces: 5 #events: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.429409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.327783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.514775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame({'a':[0,1,9,3,4]})\n",
    "to = PPObj(df, Normalize(), cont_names='a')\n",
    "to.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodes a date column. Supports multiple information by using pandas date functions. This implementation is also based on the fastai but also supports relative duration from the first event of a case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _make_date(df, date_field):\n",
    "    \"Make sure `df[date_field]` is of the right date type.\"\n",
    "    field_dtype = df[date_field].dtype\n",
    "    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        field_dtype = np.datetime64\n",
    "    if not np.issubdtype(field_dtype, np.datetime64):\n",
    "        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True,utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fu    datetime64[ns, UTC]\n",
       "dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'fu': ['2019-12-04', '2019-11-29', '2019-11-15', '2019-10-24']})\n",
    "_make_date(df, 'fu')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _secSinceSunNoon(datTimStr):\n",
    "    dt = pd.to_datetime(datTimStr).dt\n",
    "    return (dt.dayofweek-1)*24*3600+ dt.hour * 3600 + dt.minute * 60 + dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _secSinceNoon(datTimStr):\n",
    "    dt = pd.to_datetime(datTimStr).dt\n",
    "    return dt.hour * 3600 + dt.minute * 60 + dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "Base_Date_Encodings=['Year', 'Month', 'Day', 'Dayofweek', 'Dayofyear','Elapsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def encode_date(df, field_name,unit=1e9,date_encodings=Base_Date_Encodings):\n",
    "    \"Helper function that adds columns relevant to a date in the column `field_name` of `df`.\"\n",
    "    _make_date(df, field_name)\n",
    "    field = df[field_name]\n",
    "    prefix =  re.sub('[Dd]ate$', '', field_name+\"_\")\n",
    "    attr = ['Year', 'Month', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start',\n",
    "            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr:\n",
    "        if n in date_encodings: df[prefix + n] = getattr(field.dt, n.lower())\n",
    "    # Pandas removed `dt.week` in v1.1.10\n",
    "\n",
    "    if 'secSinceSunNoon' in date_encodings:\n",
    "        df[prefix+'secSinceSunNoon']=_secSinceSunNoon(field)\n",
    "    if 'secSinceNoon' in date_encodings:\n",
    "        df[prefix+'secSinceNoon']=_secSinceNoon(field)\n",
    "    if 'Week' in date_encodings:\n",
    "        week = field.dt.isocalendar().week if hasattr(field.dt, 'isocalendar') else field.dt.week\n",
    "        df.insert(3, prefix+'Week', week)\n",
    "    mask = ~field.isna()\n",
    "    elapsed = pd.Series(np.where(mask,field.values.astype(np.int64) // unit,None).astype(float),index=field.index)\n",
    "\n",
    "    if 'Relative_elapsed' in date_encodings:\n",
    "        df[prefix+'Relative_elapsed']=elapsed-elapsed.groupby(elapsed.index).transform('min')\n",
    "\n",
    "    # required to decode!\n",
    "    if 'Elapsed' in date_encodings: df[prefix+'Elapsed']=elapsed\n",
    "\n",
    "    df.drop(field_name, axis=1, inplace=True)\n",
    "    return [],[prefix+i for i in date_encodings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fu_Year</th>\n",
       "      <th>fu_Month</th>\n",
       "      <th>fu_Day</th>\n",
       "      <th>fu_Dayofweek</th>\n",
       "      <th>fu_Dayofyear</th>\n",
       "      <th>fu_Elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>338</td>\n",
       "      <td>1.575418e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>333</td>\n",
       "      <td>1.574986e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>319</td>\n",
       "      <td>1.573776e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>297</td>\n",
       "      <td>1.571875e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fu_Year  fu_Month  fu_Day  fu_Dayofweek  fu_Dayofyear    fu_Elapsed\n",
       "0     2019        12       4             2           338  1.575418e+09\n",
       "1     2019        11      29             4           333  1.574986e+09\n",
       "2     2019        11      15             4           319  1.573776e+09\n",
       "3     2019        10      24             3           297  1.571875e+09"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'fu': ['2019-12-04', '2019-11-29', '2019-11-15', '2019-10-24']})\n",
    "encode_date(df,'fu')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode_date(df, field_name,unit=1e9,date_encodings=Base_Date_Encodings):\n",
    "    df[field_name]=(df[field_name+'_'+'Elapsed'] * unit).astype('datetime64[ns, UTC]')\n",
    "    for c in date_encodings: del df[field_name+'_'+c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-04 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-29 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-15 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-24 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         fu\n",
       "0 2019-12-04 00:00:00+00:00\n",
       "1 2019-11-29 00:00:00+00:00\n",
       "2 2019-11-15 00:00:00+00:00\n",
       "3 2019-10-24 00:00:00+00:00"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_date(df,'fu')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Datetify(PPProc):\n",
    "    \"Encode dates, \"\n",
    "    order = 0\n",
    "\n",
    "    def __init__(self, date_encodings=['Relative_elapsed']): self.date_encodings=listify(date_encodings)\n",
    "\n",
    "    def encodes(self, o):\n",
    "        for i in o.date_names:\n",
    "            cat,cont=encode_date(o.items,i,date_encodings=self.date_encodings)\n",
    "            o.cont_names+=cont\n",
    "            o.cat_names+=cat\n",
    "# Todo: Add decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fu_secSinceSunNoon</th>\n",
       "      <th>fu_secSinceNoon</th>\n",
       "      <th>fu_Relative_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259200</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86400</td>\n",
       "      <td>0</td>\n",
       "      <td>432000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>950400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172800</td>\n",
       "      <td>0</td>\n",
       "      <td>1728000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fu_secSinceSunNoon  fu_secSinceNoon  fu_Relative_elapsed\n",
       "1              259200                0                  0.0\n",
       "1               86400                0             432000.0\n",
       "1                   0                0             950400.0\n",
       "1              172800                0            1728000.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'fu': ['2019-10-04', '2019-10-09', '2019-10-15', '2019-10-24']},index=[1,1,1,1])\n",
    "o = PPObj(df,Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed']),date_names='fu')\n",
    "o.xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fu_secSinceSunNoon</th>\n",
       "      <th>fu_secSinceNoon</th>\n",
       "      <th>fu_Relative_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.341627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.208083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.341645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.208082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.341655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.208080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.341636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.208079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fu_secSinceSunNoon  fu_secSinceNoon  fu_Relative_elapsed\n",
       "1           -1.341627              0.0            -1.208083\n",
       "1           -1.341645              0.0            -1.208082\n",
       "1           -1.341655              0.0            -1.208080\n",
       "1           -1.341636              0.0            -1.208079"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'fu': ['2019-10-04', '2019-10-09', '2019-10-15', '2019-10-24']},index=[1,1,1,1])\n",
    "o = PPObj(df,[Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed']),Normalize],date_names='fu')\n",
    "o.xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fu_secSinceSunNoon': 129600.0,\n",
       " 'fu_secSinceNoon': 0.0,\n",
       " 'fu_Relative_elapsed': 777600.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.procs.means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaling\n",
    "Calculates the MinMax scaling from a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MinMax(PPProc):\n",
    "    order=3\n",
    "\n",
    "    def setups(self, o):\n",
    "        store_attr(mins=o.xs.min(),\n",
    "                   maxs=o.xs.max())\n",
    "\n",
    "    def encodes(self, o):\n",
    "        cols=[i+'_minmax' for i in o.x_names]\n",
    "        o[cols] = o.xs.astype(float)\n",
    "        o[cols] = ((o.xs-self.mins) /(self.maxs-self.mins))\n",
    "        o[cols] = o.xs.astype(float)\n",
    "        o.cont_names=L(cols)\n",
    "        o.cat_names=L()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../data/logs/csv/mppn_ds/BPIC12_O.csv\"\n",
    "log=import_log(path)\n",
    "cat_names,cont_names,date_names=['activity', 'resource'], ['AMOUNT_REQ'], ['timestamp']\n",
    "o=PPObj(log,[Categorify,Datetify,FillMissing,MinMax],\n",
    "                     cat_names=cat_names,date_names=date_names,cont_names=cont_names,\n",
    "                     y_names=['activity','resource','timestamp_Relative_elapsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activity_minmax                      float64\n",
       "resource_minmax                      float64\n",
       "AMOUNT_REQ_minmax                    float64\n",
       "timestamp_Relative_elapsed_minmax    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.xs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One HoT Encoding\n",
    "Calculates the one-hot encoding of a column. It is required to first apply categorization on the same column, to deal with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OneHot(PPProc):\n",
    "    \"Transform the categorical variables to one-hot. Requires Categorify to deal with unseen data.\"\n",
    "    order = 3\n",
    "\n",
    "    def encodes(self, o):\n",
    "        new_cats=[]\n",
    "        for c in o.cat_names:\n",
    "            categories=[range(len(o.procs.categorify[c]))]\n",
    "            x=o[c].to_numpy()\n",
    "            ohe = OneHotEncoder(categories=categories)\n",
    "            enc=ohe.fit_transform(x.reshape(-1, 1)).toarray()\n",
    "            for i in range(enc.shape[1]):\n",
    "                new_cat=f'{c}_{i}'\n",
    "                o.items.loc[:,new_cat]=enc[:,i]\n",
    "                new_cats.append(new_cat)\n",
    "        o.cat_names=L(new_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df=import_log('../data/logs/csv/dapnn_ds/PDC2021_training/pdc2021_0000000.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "o=PPObj(event_df,[Categorify(),OneHot()],cat_names=['activity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Generation\n",
    "Here, we cover the sliding window generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _shift_columns (a,ws=3): return np.dstack(list(reversed([np.roll(a,i) for i in range(0,ws)])))[0]\n",
    "\n",
    "def windows_fast(df,event_ids,ws=5,pad=None):\n",
    "    max_trace_len=int(event_ids.max())+1\n",
    "    trace_start = np.where(event_ids == 0)[0]\n",
    "    trace_len=[trace_start[i]-trace_start[i-1] for i in range(1,len(trace_start))]+[len(df)-trace_start[-1]]\n",
    "    idx=[range(trace_start[i]+(i+1)\n",
    "               *(ws-1),trace_start[i]+trace_len[i]+(i+1)*(ws-1)-1) for i in range(len(trace_start))]\n",
    "    idx=np.array([y for x in idx for y in x])\n",
    "    trace_start = np.repeat(trace_start, ws-1)\n",
    "    tmp=np.stack([_shift_columns(np.insert(np.array(df[i]), trace_start, 0, axis=0),ws=ws) for i in list(df)]) \n",
    "    tmp=np.rollaxis(tmp,1) \n",
    "    res=tmp[idx]\n",
    "    if pad: res=np.pad(res,((0,0),(0,0),(pad-ws,0))) \n",
    "    \n",
    "    return res,np.where(event_ids != 0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df=import_log('../data/logs/csv/dapnn_ds/PDC2020_ground_truth/pdc_2020_0000000.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=PPObj(event_df,Categorify(),cat_names=['activity'],y_names='activity')\n",
    "#o=o.iloc[0]\n",
    "len(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws,idx=windows_fast(o.xs,o.event_ids,ws=5)\n",
    "ws,ws.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "The prefixes are converted to a `pytorch.Dataset` and than to a `DataLoader`\n",
    "A batch is than represented as a tuple of the form `(x cat. attr,x cont. attr, y cat. attr., y cont attr.)`. Also, categorical attributes are converted to a long tensor and continous attributes to a float tensor.\n",
    "\n",
    "If a dimensions of the batch is empty - e.g. the model does not use categorical input attributes - it is removed from the tuple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=PPObj(event_df,Categorify(),cat_names=['activity'],y_names='activity')\n",
    "ws,idx=windows_fast(o.xs,o.event_ids,ws=10)\n",
    "ws,ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PPDset(torch.utils.data.Dataset):\n",
    "    \"some doc\"\n",
    "    def __init__(self, inp):\n",
    "        store_attr('inp')\n",
    "\n",
    "    def __len__(self): return len(self.inp[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xs=tuple([i[idx]for i in self.inp[:-1]])\n",
    "        ys=tuple([i[idx]for i in self.inp[-1]])\n",
    "        #if len(ys)==1: ys=ys[0]\n",
    "        return (*xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@delegates(TfmdDL)\n",
    "def get_dls(ppo:PPObj,windows=windows_fast,outcome=False,event_id='event_id',bs=64,**kwargs):\n",
    "    \"some doc\"\n",
    "    ds=[]\n",
    "    for s in ppo.subsets():\n",
    "        wds,idx=windows(s.xs,s.event_ids)\n",
    "\n",
    "        if not outcome: y=s.ys.iloc[idx]\n",
    "        else: y=s.ys.groupby(s.items.index).transform('last').iloc[idx]\n",
    "        ycats=tensor(y[s.ycat_names].values).long()\n",
    "        yconts=tensor(y[s.ycont_names].values).float()\n",
    "        xconts=tensor(wds[:,len(s.cat_names):]).float()\n",
    "        xcats=tensor(wds[:,:len(s.cat_names)]).long()\n",
    "        xs=tuple([i for i in [xcats,xconts] if i.shape[1]>0])\n",
    "        ys=tuple([ycats[:,i] for i in range(ycats.shape[1])])+tuple([yconts[:,i] for i in range(yconts.shape[1])])\n",
    "        ds.append(PPDset((*xs,ys)))\n",
    "    return DataLoaders.from_dsets(*ds,bs=bs,device=torch.device('cuda'),**kwargs)\n",
    "PPObj.get_dls= get_dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls=o.get_dls()\n",
    "xb,yb=dls.one_batch()\n",
    "xb.shape,y[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Samples\n",
    "This section shows, how the PPObj can be used to create a DataLoader for pedictive process analytics:\n",
    "\n",
    "Next event prediction:  \n",
    "X: 'activity'   \n",
    "Y: 'activity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=import_log('../data/logs/csv/dapnn_ds/PDC2020_ground_truth/pdc_2020_0000001.csv.gz')\n",
    "o=PPObj(log,Categorify(),cat_names=['activity'],y_names='activity',splits=split_traces(event_df))\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls=o.get_dls(windows=partial(windows_fast,ws=2))\n",
    "xb,y=dls.one_batch()\n",
    "xb.shape,y[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pympp",
   "language": "python",
   "name": "pympp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

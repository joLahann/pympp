# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/06_anomaly.detect.ipynb.

# %% auto 0
__all__ = ['attr_dict', 'training_dl', 'ControlFlowModel', 'HideOutput', 'training_loop', 'train_validate', 'import_log_pdc',
           'process_test', 'predict_next_step', 'calc_anomaly_score', 'get_anomalies', 'nsp_accuracy', 'get_attr',
           'MultivariateModel', 'get_metrics', 'multi_loss_sum', 'my_loss', 'my_metric', 'multivariate_anomaly_score',
           'get_thresholds', 'multivariate_anomalies']

# %% ../../nbs/06_anomaly.detect.ipynb 4
from fastai.basics import *
from ..process import *
from fastai.callback.all import *
from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score
from fastai.tabular.model import get_emb_sz

# %% ../../nbs/06_anomaly.detect.ipynb 9
def training_dl(log,cat_names='activity',seed=45,ws=5,bs=32):
    categorify=Categorify()

    o=PPObj(log,procs=categorify,cat_names=cat_names,y_names=cat_names,splits=split_traces(log,seed=seed))
    dls=o.get_dls(windows=partial(windows_fast,ws=ws),bs=bs)
    return o,dls,categorify



# %% ../../nbs/06_anomaly.detect.ipynb 13
class ControlFlowModel(torch.nn.Module) :
    def __init__(self, o) :
        super().__init__()
        hidden=25
        vocab_act=len(o.procs.categorify['activity'])
        emb_dim_act = int(np.sqrt(vocab_act))+1

        self.emb_act = nn.Embedding(vocab_act,emb_dim_act)
        
        self.lstm_act = nn.LSTM(emb_dim_act, hidden, batch_first=True, num_layers=2)

        self.linear_act = nn.Linear(hidden, vocab_act)

    def forward(self, xcat):
        xcat=xcat[:,0]
        x_act=xcat
        x_act = self.emb_act(x_act)
        x_act,_ = self.lstm_act(x_act)
        x_act = x_act[:,-1]
        x_act = self.linear_act(x_act)
        return x_act

# %% ../../nbs/06_anomaly.detect.ipynb 20
class HideOutput:
    'A utility function that hides all outputs in a context'
    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout
        
def training_loop(learn,epoch,print_output,lr_find):
    '''
    Basic training loop that uses learning rate finder and one cycle training. 
    See fastai docs for more information
    '''
    if lr_find:
        lr=np.median([learn.lr_find(show_plot=print_output)[0] for i in range(5)])
        learn.fit_one_cycle(epoch,float(lr))
    else: learn.fit(epoch,0.01)
        
def train_validate(dls,m,metrics=accuracy,loss=F.cross_entropy,epoch=20,print_output=True,model_dir=".",lr_find=True,
                   patience=5,min_delta=0.005,show_plot=True,store_path='tmp',model_name='.model'):
    '''
    Trains a model on the training set with early stopping based on the validation loss.
    Afterwards, applies it to the test set.
    '''
    cbs = [
      EarlyStoppingCallback(monitor='valid_loss',min_delta=min_delta, patience=patience),
      SaveModelCallback(fname=model_name),
      ]
    learn=Learner(dls, m, path=store_path, model_dir=model_dir, loss_func=loss ,metrics=metrics,cbs=cbs)

    if print_output:
        training_loop(learn,epoch,show_plot,lr_find=lr_find)
        return learn.validate(dl=dls[2])
    else:
        with HideOutput(),learn.no_bar(),learn.no_logging():
            training_loop(learn,epoch,show_plot,lr_find=lr_find)
            return learn.validate(dl=dls[2])

# %% ../../nbs/06_anomaly.detect.ipynb 24
def import_log_pdc(log):
    df = import_log(log)
    log = df.rename({"case:pdc:isPos":'normal'},axis=1,inplace=True)
    return df

# %% ../../nbs/06_anomaly.detect.ipynb 27
def process_test(test_log,categorify,cat_names='activity'):
    o=PPObj(test_log,procs=categorify,cat_names=cat_names,y_names=cat_names,do_setup=False)
    o.process()
    return o


# %% ../../nbs/06_anomaly.detect.ipynb 30
def predict_next_step(o,m,ws=5):
    wds,idx=partial(windows_fast,ws=ws)(o.xs, o.event_ids)
    res=(m(LongTensor(wds).cuda()))
    return res,idx

# %% ../../nbs/06_anomaly.detect.ipynb 33
def calc_anomaly_score(res,o,idx):
    sm = nn.Softmax(dim=1)
    y = o.items['activity'].iloc[idx].values
    p = sm(res)
    pred = p.max(1)[0]
    truth = p[list(range(0, len(y))),y]
    a_score = ((pred - truth) / pred).cpu().detach().numpy()
    return a_score

# %% ../../nbs/06_anomaly.detect.ipynb 36
def get_anomalies(a_score,o,idx,threshhold=0.98):
    df=pd.DataFrame(columns=[ 'a_score'])
    df['a_score'] = a_score
    df['trace_id'] = o.items.iloc[idx].index.values
    df['normal'] = o.items.iloc[idx]['normal'].values
    df

    y_true = (df.loc[df.trace_id.drop_duplicates().index].normal==False).tolist()
    cases = df.loc[df.trace_id.drop_duplicates().index].trace_id.tolist()
    anomalies = set(list(df.loc[df['a_score'] > threshhold]['trace_id']))
    y_pred=[case in anomalies for case in cases]
    return y_pred,y_true

# %% ../../nbs/06_anomaly.detect.ipynb 43
def nsp_accuracy(o,idx,nsp):
    nsp_y=o.ys.iloc[idx]
    nsp_acc= accuracy(nsp,tensor(nsp_y.values).cuda())
    return nsp_acc

# %% ../../nbs/06_anomaly.detect.ipynb 47
attr_dict={}
for i in [i for i in glob.glob('data/logs/csv/dapnn_ds/binet_logs/*')]:
    i=Path(i).stem[:-4]
    if 'bpic12'in i:
        attr_dict[i] =['activity']
    elif 'bpic13' in i:
        attr_dict[i]=['activity','org:group',
'org:resource', 'org:role', 'organization country', 'product', 'resource country','impact']
    elif 'bpic17' in i:  
        attr_dict[i]=['activity','EventOrigin','org:resource']
    elif 'bpic15' in i:
        attr_dict[i]=['activity', 'action_code', 'activityNameEN', 'activityNameNL','monitoringResource', 'org:resource', 'question']
    elif '-1' in i:
        attr_dict[i]=['activity','user']
    elif '-2' in i:
        attr_dict[i]=['activity','user','day']
    elif '-3' in i:
        attr_dict[i]=['activity','user','day','country']
    elif '-4' in i:
        attr_dict[i]=['activity','user','day','country','company']

# %% ../../nbs/06_anomaly.detect.ipynb 49
def get_attr(attr_dict,log_name):
    if log_name in attr_dict: 
        return attr_dict[log_name]
    else: return ['activity']

# %% ../../nbs/06_anomaly.detect.ipynb 56
class MultivariateModel(torch.nn.Module) :
    def __init__(self, emb_szs, lstm_neurons=25,lstm_layers=2) :
        super().__init__()
        
        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])
        self.lstms = nn.ModuleList([nn.LSTM(nf, lstm_neurons, batch_first=True, num_layers=lstm_layers) 
                                    for ni,nf in emb_szs])
        self.linears = nn.ModuleList([nn.Linear(lstm_neurons, ni) for ni,nf in emb_szs])

        

    def forward(self, xcat):
        res=[]
        for i in range(xcat.shape[1]):
            x =xcat[:,i]
            x =self.embeds[i](x)
            x,_ =self.lstms[i](x)
            x = x[:,-1]
            x= self.linears[i](x)
            res.append(x)
        return tuple(res)
    


# %% ../../nbs/06_anomaly.detect.ipynb 58
def _accuracy_idx(a,b,i): return accuracy(listify(a)[i],listify(b)[i])

def get_metrics(o):
    
    number_cats=len(o.ycat_names)
    
    accuracies=[]
    for i in range(number_cats):
        accuracy_func=partial(_accuracy_idx,i=i)
        accuracy_func.__name__= f"acc_{o.ycat_names[i]}"
        accuracy_func=AvgMetric(accuracy_func)
        accuracies.append(accuracy_func)
    mae_days=None
    return L(accuracies)+mae_days

def multi_loss_sum(o,p,y):
    '''Multi Loss function that sums up multiple loss functions. The selection of the loss function is based on the PPObj o'''
    p,y=listify(p),listify(y)
    len_cat,len_cont=len(o.ycat_names),len(o.ycont_names)
    cross_entropies=[F.cross_entropy(p[i],y[i]) for i in range(len_cat)]
    return torch.sum(torch.stack(list(L(cross_entropies))))

def my_loss(p,y): return F.cross_entropy(p[0],y[0])
def my_metric(p,y): return accuracy(p[0],y[0])

# %% ../../nbs/06_anomaly.detect.ipynb 64
def predict_next_step(o,m,ws=5):
    wds,idx=partial(windows_fast,ws=ws)(o.xs, o.event_ids)
    res= []
    with torch.no_grad():
        for b in DataLoader(wds,bs=8*1024,shuffle=False):
            h= m(b.long().cuda())
            h= tuple([i.cpu() for i in h])
            res.append(h)

    res =tuple([torch.cat([k[i] for k in res] ) for i in range(len(o.cat_names))])
    return res,idx

# %% ../../nbs/06_anomaly.detect.ipynb 66
def multivariate_anomaly_score(res,o,idx,cols):
    score_df=pd.DataFrame()

    for cidx,_ in enumerate(cols):
        sm = nn.Softmax(dim=1)
        p = sm(res[cidx].cpu())
        pred = p.max(1)[0]
        y = o.items[cols[cidx]].iloc[idx].values

        truth=p[list(range(len(y))),y]
        score = ((pred - truth) / pred).tolist()
        score_df[cols[cidx]] = score
    score_df['trace_id']=o.items.index.to_series().iloc[idx].values
    return score_df

# %% ../../nbs/06_anomaly.detect.ipynb 70
def get_thresholds(col,act_threshold=0.964,attr_threshold=0.9971):
    """
    Defines a custom threshold function
    """
    if col =='activity':
        return act_threshold
    else: 
        return attr_threshold

# %% ../../nbs/06_anomaly.detect.ipynb 71
def multivariate_anomalies(score_df,cols,idx,o,anomaly_col='normal',fixed_threshold=None,get_thresholds=get_thresholds):
    if fixed_threshold is None:
        comp_thresholds=get_thresholds
    else:
        comp_thresholds = lambda _: fixed_threshold
    a=[score_df.loc[score_df[c] >= comp_thresholds(c)]['trace_id'] for c in cols]
    anomalies=list(set().union(*a))
    h=o.items.iloc[idx][anomaly_col]
    if anomaly_col=='anomaly':
        score_df['is_anomaly']=(h!='normal').tolist()
    else:
        score_df['is_anomaly']=(h==False).tolist()
    score_df['anomaly']=h.tolist()
    y_true = (score_df.loc[score_df.trace_id.drop_duplicates().index].is_anomaly).tolist()
    cases = score_df.loc[score_df.trace_id.drop_duplicates().index].trace_id.tolist()
    y_pred=[case in anomalies for case in cases]
    return y_true,y_pred

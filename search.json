[
  {
    "objectID": "10_anomaly.performance_binet.html",
    "href": "10_anomaly.performance_binet.html",
    "title": "BINET Performance",
    "section": "",
    "text": "from fastai.basics import *\nfrom pympp.process import *\nfrom pympp.anomaly.detect import *\nfrom pympp.anomaly.heuristics import *\nfrom sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\nimport seaborn as sns\nfrom fastai.tabular.model import get_emb_sz\nfrom bayes_opt import BayesianOptimization"
  },
  {
    "objectID": "10_anomaly.performance_binet.html#load-logs-and-models",
    "href": "10_anomaly.performance_binet.html#load-logs-and-models",
    "title": "BINET Performance",
    "section": "Load Logs and Models",
    "text": "Load Logs and Models\n\nlogs = [Path(fn).stem[:-4] for fn in progress_bar(glob.glob(f'../data/logs/csv/dapnn_ds/binet_logs/*'))]\nlen(logs)\n\n\n\n\n\n\n    \n      \n      100.00% [40/40 00:00&lt;00:00]\n    \n    \n\n\n40\n\n\n\nlearner_path=f'models/binet_logs/'\n\n\ndef load_pred_model(learner_path,train_log_path,log_name,cols=['activity']):\n    p = f'{learner_path}/{log_name}_vocab.p'\n    with open(p, 'rb') as fp:\n        categorify = pickle.load(fp)\n    log = import_log(train_log_path)\n    o = process_test(log,categorify,cols)\n    dls=o.get_dls()\n    loss=partial(multi_loss_sum,o)\n    emb_szs = get_emb_sz(o)\n    m=MultivariateModel(emb_szs)\n    learn=Learner(dls, m, path=learner_path, model_dir='.', loss_func=loss, metrics=get_metrics(o))\n    learn.load(log_name,with_opt=False)\n    m=learn.model.cuda()\n    return m, categorify\n\n\ndef multivariate_anomaly_score(res,o,idx,cols):\n    score_df=pd.DataFrame()\n\n    for cidx,_ in enumerate(cols):\n        sm = nn.Softmax(dim=1)\n        p = sm(res[cidx].cpu())\n        pred = p.max(1)[0]\n        y = o.items[cols[cidx]].iloc[idx].values\n\n        truth=p[list(range(len(y))),y]\n        score = ((pred - truth) / pred).tolist()\n        score_df[cols[cidx]] = score\n    score_df['trace_id']=o.items.index.to_series().iloc[idx].values\n    return score_df\n\n\ndef get_score_df(log_name):\n    fn=f'./data/logs/csv/dapnn_ds/binet_logs/{log_name}.csv.gz'\n    cols= get_attr(attr_dict,log_name)\n    training_log_path=f'./data/logs/csv/dapnn_ds/binet_logs/{log_name}.csv.gz'\n    m2, categorify = load_pred_model(learner_path,training_log_path,log_name,cols=cols)\n    log = import_log(fn,cols)\n    o = process_test(log,categorify,cols)\n    res,idx=predict_next_step(o,m2)\n    score_df=multivariate_anomaly_score(res,o,idx,cols)\n    return score_df,cols,o\n\n\nlog_name= 'bpic15-0.3-4'\nscore_df,cols,o = get_score_df(log_name)\n\n/tmp/ipykernel_3414969/1108395208.py:2: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\nmax_error_cases = score_df.groupby('trace_id').max()\ny_true =o.items.groupby(o.items.index)['anomaly'].last().to_numpy() != 'normal'\n\n\nall_cols = np.unique([i for j in attr_dict.values() for i in j ])"
  },
  {
    "objectID": "10_anomaly.performance_binet.html#best-heuristic---bayesian-optimization",
    "href": "10_anomaly.performance_binet.html#best-heuristic---bayesian-optimization",
    "title": "BINET Performance",
    "section": "Best Heuristic - Bayesian Optimization",
    "text": "Best Heuristic - Bayesian Optimization\n\nall_cols =['activity', 'user', 'day', 'country', 'company','org:resource',]\n\n\ndef optimize(**settings):\n    y_pred =(np.array([(max_error_cases[c]&gt;settings[c]).tolist() for c in cols]).max(axis=0))   \n    return (f1_score(y_pred,y_true))\n\n# Bounded region of parameter space\npbounds = {c:(0.9,1.0) for c in cols}\n\n\n# Only run this if you want to recreate the best_th_df, takes arround 3h\nres = []\nn_iter = 500\n\nfor log_name in progress_bar(logs):\n    score_df,cols,o = get_score_df(log_name)\n    max_error_cases = score_df.groupby('trace_id').max()\n    y_true =o.items.groupby(o.items.index)['anomaly'].last().to_numpy() != 'normal'\n    pbounds = {c:(0.8,1.0) for c in cols}\n\n    optimizer = BayesianOptimization(\n        f=optimize,\n        pbounds=pbounds,\n        verbose=0,\n        random_state=1,\n    )\n    optimizer.maximize(init_points=20, n_iter=n_iter) \n    res.append([log_name,optimizer.max['target']]+\n               [optimizer.max['params'][c] if c in cols else 0.0 for c in all_cols ])\n\n\n\n\n\n\n    \n      \n      100.00% [40/40 00:57&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_3414969/1864857608.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/tmp/ipykernel_3414969/1864857608.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/1864857608.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/1864857608.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/1864857608.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\nbest_th_df = pd.DataFrame(res,columns=['log_name','best F1 Score',*all_cols])\nbest_th_df\n\n\n\n\n\n\n\n\nlog_name\nbest F1 Score\nactivity\nuser\nday\ncountry\ncompany\norg:resource\n\n\n\n\n0\nbpic15-0.3-4\n0.451471\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n1\nlarge-0.3-4\n0.937398\n0.940005\n1.000000\n1.000000\n0.999750\n0.981446\n0.000000\n\n\n2\nsmall-0.3-4\n0.961668\n0.873245\n1.000000\n1.000000\n0.931696\n1.000000\n0.000000\n\n\n3\nbpic13-0.3-3\n0.419858\n0.824835\n0.000000\n0.000000\n0.000000\n0.000000\n0.993919\n\n\n4\nmedium-0.3-3\n0.930289\n0.823992\n0.972513\n0.964294\n0.960638\n0.000000\n0.000000\n\n\n5\nmedium-0.3-1\n0.975855\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n6\nbpic17-0.3-1\n0.484787\n0.893105\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n7\nwide-0.3-3\n0.926388\n0.847682\n1.000000\n0.946180\n0.995507\n0.000000\n0.000000\n\n\n8\nlarge-0.3-2\n0.951917\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n9\nbpic17-0.3-2\n0.838753\n0.884222\n0.000000\n0.000000\n0.000000\n0.000000\n0.991578\n\n\n10\nbpic13-0.3-1\n0.468346\n0.824835\n0.000000\n0.000000\n0.000000\n0.000000\n0.993919\n\n\n11\nsmall-0.3-3\n0.932209\n0.941794\n0.960505\n0.932191\n0.948540\n0.000000\n0.000000\n\n\n12\nbpic12-0.3-1\n0.717622\n0.975623\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n13\nhuge-0.3-2\n0.904886\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n14\np2p-0.3-3\n0.957556\n0.931714\n0.940759\n1.000000\n1.000000\n0.000000\n0.000000\n\n\n15\nwide-0.3-4\n0.887997\n0.885476\n1.000000\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n16\nhuge-0.3-1\n0.984322\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n17\ngigantic-0.3-1\n0.907391\n0.937300\n0.966925\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n18\np2p-0.3-1\n0.993802\n0.975278\n0.978921\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n19\nwide-0.3-2\n0.958913\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n20\nbpic15-0.3-2\n0.436090\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n21\nlarge-0.3-1\n0.987251\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n22\nbpic15-0.3-1\n0.490875\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n23\ngigantic-0.3-3\n0.936312\n0.895252\n1.000000\n0.976421\n1.000000\n0.000000\n0.000000\n\n\n24\nmedium-0.3-4\n0.931641\n0.940106\n1.000000\n1.000000\n0.984717\n1.000000\n0.000000\n\n\n25\nmedium-0.3-2\n0.935906\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n26\nhuge-0.3-4\n0.887196\n0.896746\n1.000000\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n27\ngigantic-0.3-4\n0.879924\n0.926454\n1.000000\n0.998477\n0.981558\n1.000000\n0.000000\n\n\n28\nbpic13-0.3-2\n0.485372\n0.800000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n29\np2p-0.3-2\n0.975610\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n30\nbpic15-0.3-3\n0.474283\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n31\nbpic15-0.3-5\n0.439946\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n32\nlarge-0.3-3\n0.892308\n0.867383\n1.000000\n0.978184\n0.996034\n0.000000\n0.000000\n\n\n33\nsmall-0.3-2\n0.973026\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n34\nhuge-0.3-3\n0.824321\n0.960149\n0.938465\n0.862685\n0.993652\n0.000000\n0.000000\n\n\n35\nwide-0.3-1\n0.991540\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n36\npaper-0.3-1\n0.999659\n0.883404\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n37\nsmall-0.3-1\n0.996956\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n38\ngigantic-0.3-2\n0.884754\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n39\np2p-0.3-4\n0.981248\n0.946049\n0.995983\n1.000000\n1.000000\n0.957152\n0.000000"
  },
  {
    "objectID": "10_anomaly.performance_binet.html#other-heuristics",
    "href": "10_anomaly.performance_binet.html#other-heuristics",
    "title": "BINET Performance",
    "section": "Other Heuristics",
    "text": "Other Heuristics\n\ndef multi_attr_f1_score(y_true,max_error_cases,ths):\n    y_pred =(np.array([(max_error_cases[c]&gt;ths[c]).tolist() for c in cols]).max(axis=0))   \n    return f1_score(y_pred,y_true)\n\n\ndef threshold_anomaly_ratio_df(max_error_cases,min_th= 0.0):\n    res = []\n    for col in cols:\n        ths = max_error_cases[col].unique()\n        ths = ths[ths &gt;min_th]\n        r = np.array([sum(max_error_cases[col] &gt;th) for th in ths])/len(max_error_cases)\n        res+=zip(ths,r,[col]*len(ths))\n    return pd.DataFrame(res,columns=['Th','Anomaly Ratio','Col']) \n\ndef threshold_anomaly_ratio_df_fixed(max_error_cases):\n    ths =np.arange(100)*0.001+0.9\n    res=[]\n    for col in cols:\n        r = np.array([sum(max_error_cases[col] &gt;th) for th in ths])/len(max_error_cases)\n        res+=zip(ths,r,[col]*len(ths))\n    return pd.DataFrame(res,columns=['Th','Anomaly Ratio','Col'])\n\n\nheuristics = [partial(get_ratio_th,0.5),elbow_heuristic,\n              partial(get_fixed_heuristic,0.98),get_lowest_plateau_heuristic]\nheuristic_names = ['r_0.5','e_down','e_up','fix','lp_min','lp_mean','lp_max']\n\n\nlog_name='paper-0.3-1'\n\nscore_df,cols,o = get_score_df(log_name)\nmax_error_cases = score_df.groupby('trace_id').max()\ny_true =o.items.groupby(o.items.index)['anomaly'].last().to_numpy() != 'normal'\ndf =threshold_anomaly_ratio_df_fixed(max_error_cases)\nmulti_attr_f1_score(y_true,max_error_cases,{'activity':0.945,'user':0.984})\n\n0.9962341663813762\n\n\n\nget_lowest_plateau_heuristic(df[df['Col']=='user'])\n\n(Th                0.985\n Anomaly Ratio    0.1862\n Col                user\n Name: 185, dtype: object,\n Th                0.985\n Anomaly Ratio    0.1862\n Col                user\n Name: 185, dtype: object,\n Th                0.985\n Anomaly Ratio    0.1862\n Col                user\n Name: 185, dtype: object)\n\n\n\ntest = df[df['Col']=='activity'].copy()\ntest.index = test['Th']\n\n\nget_lowest_plateau_heuristic(test)\n\n(Th                  0.901\n Anomaly Ratio      0.2462\n Col              activity\n Name: 0.901, dtype: object,\n Th                  0.944\n Anomaly Ratio      0.2462\n Col              activity\n Name: 0.9440000000000001, dtype: object,\n Th                  0.988\n Anomaly Ratio      0.2462\n Col              activity\n Name: 0.988, dtype: object)\n\n\n\nlog_name='paper-0.3-1'\nscore_df,cols,o = get_score_df(log_name)\nmax_error_cases = score_df.groupby('trace_id').max()\ny_true =o.items.groupby(o.items.index)['anomaly'].last().to_numpy() != 'normal'\ndf =threshold_anomaly_ratio_df_fixed(max_error_cases)\n#df =threshold_anomaly_ratio_df(max_error_cases)\n\nh_ths = {col:[i for j in [[k.Th for k in listify(h(df[df['Col']==col]))] for h in heuristics] for i in j] for col in cols}\nh_ths_df = pd.DataFrame(h_ths,index=heuristic_names).transpose()\nh_ths_df['Log'] = log_name\nh_ths_df\n\n\n\n\n\n\n\n\nr_0.5\ne_down\ne_up\nfix\nlp_min\nlp_mean\nlp_max\nLog\n\n\n\n\nactivity\n0.9\n0.992\n0.990\n0.901\n0.901\n0.944\n0.988\npaper-0.3-1\n\n\nuser\n0.9\n0.912\n0.911\n0.900\n0.985\n0.985\n0.985\npaper-0.3-1\n\n\n\n\n\n\n\n\nf1_scores={h:multi_attr_f1_score(y_true,max_error_cases,h_ths_df[h]) for h in heuristic_names}\nf1_scores\n\n{'r_0.5': 0.8982219497240956,\n 'e_down': 0.9602836879432624,\n 'e_up': 0.8998459167950694,\n 'fix': 0.8982219497240956,\n 'lp_min': 0.9962341663813762,\n 'lp_mean': 0.9962341663813762,\n 'lp_max': 0.9962341663813762}\n\n\n\nres = []\nres_ths = []\nfor log_name in progress_bar(logs):\n    score_df,cols,o = get_score_df(log_name)\n    max_error_cases = score_df.groupby('trace_id').max()\n    y_true =o.items.groupby(o.items.index)['anomaly'].last().to_numpy() != 'normal'\n    df =threshold_anomaly_ratio_df_fixed(max_error_cases)\n    #df =threshold_anomaly_ratio_df(max_error_cases)\n    h_ths = {col:[i for j in [[k.Th for k in listify(h(df[df['Col']==col]))] for h in heuristics] for i in j] for col in cols}\n    h_ths_df = pd.DataFrame(h_ths,index=heuristic_names).transpose()\n    h_ths_df['Log'] = log_name\n    res_ths+=h_ths_df.values.tolist()\n\n    f1_scores={h:multi_attr_f1_score(y_true,max_error_cases,h_ths_df[h]) for h in heuristic_names}\n    f1_scores['Log']= log_name\n    res.append(f1_scores)\nheuristic_th_df = pd.DataFrame(res)\nheuristic_th_df\n\n\n\n\n\n\n    \n      \n      100.00% [40/40 00:59&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_3414969/2538357640.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/2538357640.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/2538357640.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/2538357640.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/2538357640.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\n\n\n\n\n\n\nr_0.5\ne_down\ne_up\nfix\nlp_min\nlp_mean\nlp_max\nLog\n\n\n\n\n0\n0.469136\n0.470405\n0.450185\n0.451471\n0.469498\n0.469498\n0.469498\nbpic15-0.3-4\n\n\n1\n0.683812\n0.768873\n0.912346\n0.683812\n0.907368\n0.907923\n0.908979\nlarge-0.3-4\n\n\n2\n0.709933\n0.950145\n0.943371\n0.709933\n0.943084\n0.943084\n0.943084\nsmall-0.3-4\n\n\n3\n0.391753\n0.395604\n0.397590\n0.377088\n0.421603\n0.421603\n0.419580\nbpic13-0.3-3\n\n\n4\n0.781832\n0.797542\n0.830393\n0.781832\n0.870879\n0.870879\n0.870879\nmedium-0.3-3\n\n\n5\n0.891285\n0.881666\n0.912883\n0.891285\n0.936762\n0.936674\n0.935889\nmedium-0.3-1\n\n\n6\n0.544370\n0.468426\n0.525774\n0.459481\n0.459481\n0.459481\n0.461569\nbpic17-0.3-1\n\n\n7\n0.758085\n0.916066\n0.806943\n0.758085\n0.889936\n0.889936\n0.890236\nwide-0.3-3\n\n\n8\n0.809877\n0.892727\n0.876196\n0.809877\n0.978667\n0.978667\n0.978667\nlarge-0.3-2\n\n\n9\n0.694332\n0.669278\n0.858127\n0.670450\n0.668974\n0.668974\n0.668974\nbpic17-0.3-2\n\n\n10\n0.449840\n0.437579\n0.440025\n0.435620\n0.472403\n0.472403\n0.472403\nbpic13-0.3-1\n\n\n11\n0.905615\n0.965012\n0.981543\n0.905615\n0.986047\n0.988345\n0.988345\nsmall-0.3-3\n\n\n12\n0.659213\n0.694447\n0.721668\n0.620128\n0.689842\n0.687652\n0.684029\nbpic12-0.3-1\n\n\n13\n0.805017\n0.916031\n0.941257\n0.805017\n0.939937\n0.939937\n0.939937\nhuge-0.3-2\n\n\n14\n0.848600\n0.914738\n0.871881\n0.848600\n0.976789\n0.976789\n0.976789\np2p-0.3-3\n\n\n15\n0.770405\n0.912192\n0.912023\n0.770405\n0.948236\n0.948236\n0.948236\nwide-0.3-4\n\n\n16\n0.837684\n0.868746\n0.947512\n0.837684\n0.975577\n0.975234\n0.975234\nhuge-0.3-1\n\n\n17\n0.813358\n0.781813\n0.928790\n0.813358\n0.790813\n0.790813\n0.790385\ngigantic-0.3-1\n\n\n18\n0.913346\n0.991741\n0.963235\n0.913346\n0.957545\n0.957173\n0.957143\np2p-0.3-1\n\n\n19\n0.856201\n0.961786\n0.926232\n0.856201\n0.972232\n0.972232\n0.972232\nwide-0.3-2\n\n\n20\n0.454545\n0.436911\n0.436090\n0.436090\n0.441261\n0.441261\n0.441261\nbpic15-0.3-2\n\n\n21\n0.857063\n0.882013\n0.865281\n0.857063\n0.948518\n0.949672\n0.951158\nlarge-0.3-1\n\n\n22\n0.518569\n0.500963\n0.493671\n0.490875\n0.509521\n0.509521\n0.509521\nbpic15-0.3-1\n\n\n23\n0.718496\n0.772625\n0.778030\n0.718496\n0.857563\n0.857563\n0.857563\ngigantic-0.3-3\n\n\n24\n0.775132\n0.838451\n0.869688\n0.779234\n0.893204\n0.893204\n0.893204\nmedium-0.3-4\n\n\n25\n0.815190\n0.872786\n0.885749\n0.815190\n0.925835\n0.925835\n0.926812\nmedium-0.3-2\n\n\n26\n0.721685\n0.810028\n0.847239\n0.721685\n0.920584\n0.921463\n0.922636\nhuge-0.3-4\n\n\n27\n0.630247\n0.664441\n0.632649\n0.629715\n0.771056\n0.771056\n0.771280\ngigantic-0.3-4\n\n\n28\n0.489189\n0.484236\n0.492133\n0.469314\n0.503036\n0.503036\n0.503036\nbpic13-0.3-2\n\n\n29\n0.867094\n0.980174\n0.975421\n0.867094\n0.961525\n0.961525\n0.961525\np2p-0.3-2\n\n\n30\n0.499408\n0.483711\n0.478950\n0.474283\n0.500573\n0.500573\n0.500573\nbpic15-0.3-3\n\n\n31\n0.471927\n0.442779\n0.438893\n0.439946\n0.463188\n0.463188\n0.463519\nbpic15-0.3-5\n\n\n32\n0.642612\n0.700446\n0.662090\n0.642612\n0.949521\n0.949521\n0.948849\nlarge-0.3-3\n\n\n33\n0.853288\n0.880489\n0.858198\n0.853288\n0.959727\n0.959727\n0.959727\nsmall-0.3-2\n\n\n34\n0.744374\n0.799891\n0.820757\n0.744374\n0.948801\n0.948801\n0.949416\nhuge-0.3-3\n\n\n35\n0.850430\n0.875817\n0.857226\n0.850430\n0.975823\n0.975823\n0.975823\nwide-0.3-1\n\n\n36\n0.898222\n0.960284\n0.899846\n0.898222\n0.996234\n0.996234\n0.996234\npaper-0.3-1\n\n\n37\n0.911104\n0.978709\n0.995289\n0.911104\n0.995289\n0.995289\n0.995289\nsmall-0.3-1\n\n\n38\n0.795745\n0.838137\n0.884650\n0.795745\n0.822642\n0.822642\n0.822924\ngigantic-0.3-2\n\n\n39\n0.744456\n0.860472\n0.830008\n0.744456\n0.978738\n0.979074\n0.980419\np2p-0.3-4\n\n\n\n\n\n\n\n\nheuristic_th_df.describe()\n\n\n\n\n\n\n\n\nr_0.5\ne_down\ne_up\nfix\nlp_min\nlp_mean\nlp_max\n\n\n\n\ncount\n40.000000\n40.000000\n40.000000\n40.000000\n40.000000\n40.000000\n40.000000\n\n\nmean\n0.721312\n0.767954\n0.778746\n0.713463\n0.814458\n0.814513\n0.814571\n\n\nstd\n0.153024\n0.188515\n0.189436\n0.163644\n0.201974\n0.202089\n0.202227\n\n\nmin\n0.391753\n0.395604\n0.397590\n0.377088\n0.421603\n0.421603\n0.419580\n\n\n25%\n0.639520\n0.668069\n0.654730\n0.627319\n0.684625\n0.682983\n0.680265\n\n\n50%\n0.764245\n0.838294\n0.858163\n0.764245\n0.923210\n0.923649\n0.924724\n\n\n75%\n0.849058\n0.912828\n0.912480\n0.849058\n0.960176\n0.960176\n0.960176\n\n\nmax\n0.913346\n0.991741\n0.995289\n0.913346\n0.996234\n0.996234\n0.996234\n\n\n\n\n\n\n\n\nheuristic_th_df['best']=best_th_df['best F1 Score']\n\n\nds=['BPIC12','BPIC13','BPIC15','BPIC17','Paper','P2P','Small','Medium','Large','Huge','Gigantic','Wide']\n\ndf =pd.DataFrame([heuristic_th_df[heuristic_th_df['Log'].str.contains(i.lower())].mean() for i in ds],index = ds)\nres_binet_df = df.copy().T\nres_binet_df\n\n\n\n\n\n\n\n\nBPIC12\nBPIC13\nBPIC15\nBPIC17\nPaper\nP2P\nSmall\nMedium\nLarge\nHuge\nGigantic\nWide\n\n\n\n\nr_0.5\n0.659213\n0.443594\n0.482717\n0.619351\n0.898222\n0.843374\n0.844985\n0.815860\n0.748341\n0.777190\n0.739462\n0.808780\n\n\ne_down\n0.694447\n0.439140\n0.466954\n0.568852\n0.960284\n0.936781\n0.943588\n0.847611\n0.811015\n0.848674\n0.764254\n0.916465\n\n\ne_up\n0.721668\n0.443249\n0.459558\n0.691950\n0.899846\n0.910137\n0.944600\n0.874679\n0.828978\n0.889191\n0.806030\n0.875606\n\n\nfix\n0.620128\n0.427341\n0.458533\n0.564966\n0.898222\n0.843374\n0.844985\n0.816885\n0.748341\n0.777190\n0.739329\n0.808780\n\n\nlp_min\n0.689842\n0.465680\n0.476808\n0.564228\n0.996234\n0.968649\n0.971037\n0.906670\n0.946018\n0.946225\n0.810518\n0.946557\n\n\nlp_mean\n0.687652\n0.465680\n0.476808\n0.564228\n0.996234\n0.968640\n0.971611\n0.906648\n0.946446\n0.946359\n0.810518\n0.946557\n\n\nlp_max\n0.684029\n0.465006\n0.476874\n0.565271\n0.996234\n0.968969\n0.971611\n0.906696\n0.946913\n0.946806\n0.810538\n0.946632\n\n\nbest\n0.717622\n0.457859\n0.458533\n0.661770\n0.999659\n0.977054\n0.965965\n0.943423\n0.942218\n0.900181\n0.902095\n0.941210\n\n\n\n\n\n\n\n\ndf['Log']=df.index\n\n\ndef get_plot_data(th_df):\n    plot_data=th_df.melt(var_name='Heuristic',value_name='F1 Score',id_vars = ['Log'],value_vars=['r_0.5',\n 'e_down',\n 'e_up',\n 'fix',\n 'lp_min',\n 'lp_mean',\n 'lp_max',\n 'best',],ignore_index=False)\n    plot_data.index=range(len(plot_data))\n    return plot_data\nplot_data = get_plot_data(df)\n\n\nsns.set_theme(style=\"darkgrid\")\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n\ng = sns.catplot(\n    data=plot_data, kind=\"bar\",\n    x=\"Log\", y=\"F1 Score\", hue=\"Heuristic\",\n    errorbar=\"sd\", palette=\"dark\", alpha=.6, height=6,aspect=2\n)\ng.despine(left=True)\ng.set_axis_labels(\"LOGS\", \"F1 SCORE\")\nplt.xticks(rotation = 0)\ng.legend.set_title(\"\")\n\n\n\n\n\nbinet_synth_df =heuristic_th_df.sort_values('best')\n\ndef get_plot_data(th_df,id_vars=['Log Name'],value_vars = ['F1 Score','Anomaly Ratio']):\n    plot_data=th_df.melt(var_name='Heuristic',value_name='Score',id_vars =id_vars ,value_vars=value_vars,ignore_index=False)\n    plot_data['Threshold']=plot_data.index\n    plot_data.index=range(len(plot_data))\n    return plot_data\n\nplot_data = get_plot_data(binet_synth_df,id_vars=['Log'],value_vars=['best',\n 'e_down',\n 'fix',\n 'lp_mean',\n])\n\n\nlog2int = {j:i for i,j in enumerate(binet_synth_df['Log'].unique())}\nplot_data ['Log']= plot_data['Log'].map(log2int)\nplot_data ['F1 Score']=plot_data ['Score']\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\ng=sns.scatterplot(data =plot_data, x='Log',y='F1 Score', hue='Heuristic' ,linewidth=0,)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\ng.set_xticks(range(len(binet_synth_df)));\ng.set_xticklabels(binet_synth_df['Log'].unique(),rotation = 90);"
  },
  {
    "objectID": "10_anomaly.performance_binet.html#anomaly-types",
    "href": "10_anomaly.performance_binet.html#anomaly-types",
    "title": "BINET Performance",
    "section": "Anomaly Types",
    "text": "Anomaly Types\n\nh_ths_df['lp_mean']\n\nactivity    0.986\nuser        0.985\nday         0.987\ncountry     0.985\ncompany     0.981\nName: lp_mean, dtype: float64\n\n\n\nlog_name = 'large-0.3-4'\nscore_df,cols,o = get_score_df(log_name)\nmax_error_cases = score_df.groupby('trace_id').max()\ny_true =o.items.groupby(o.items.index)['anomaly'].last().to_numpy() != 'normal'\ndf =threshold_anomaly_ratio_df_fixed(max_error_cases)\nh_ths = {col:[i for j in [[k.Th for k in listify(h(df[df['Col']==col]))] for h in heuristics] for i in j] for col in cols}\nh_ths_df = pd.DataFrame(h_ths,index=heuristic_names).transpose()\nths = h_ths_df['lp_mean']\ny_pred =(np.array([(max_error_cases[c]&gt;ths[c]).tolist() for c in cols]).max(axis=0))   \nk=o.items.groupby(o.items.index)['anomaly'].last().iloc[np.where(np.equal(y_pred, y_true))[0]]\nk.value_counts()\n\nnormal          3215\nEarly            267\nInsert           262\nSkipSequence     259\nRework           251\nLate             239\nAttribute        206\nName: anomaly, dtype: int64\n\n\n\nk=o.items.groupby(o.items.index)['anomaly'].last().iloc[np.where(~np.equal(y_pred, y_true))[0]]\nk.value_counts()\nmulti_attr_f1_score(y_true,max_error_cases,ths.to_dict())\n\n0.9079229122055674\n\n\n\nths\n\nactivity    0.981\nuser        0.978\nday         0.984\ncountry     0.984\ncompany     0.975\nName: lp_mean, dtype: float64\n\n\n\nbest_th_df\n\n\n\n\n\n\n\n\nlog_name\nbest F1 Score\nactivity\nuser\nday\ncountry\ncompany\norg:resource\n\n\n\n\n0\nbpic15-0.3-4\n0.451471\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n1\nlarge-0.3-4\n0.937398\n0.940005\n1.000000\n1.000000\n0.999750\n0.981446\n0.000000\n\n\n2\nsmall-0.3-4\n0.961668\n0.873245\n1.000000\n1.000000\n0.931696\n1.000000\n0.000000\n\n\n3\nbpic13-0.3-3\n0.419858\n0.824835\n0.000000\n0.000000\n0.000000\n0.000000\n0.993919\n\n\n4\nmedium-0.3-3\n0.930289\n0.823992\n0.972513\n0.964294\n0.960638\n0.000000\n0.000000\n\n\n5\nmedium-0.3-1\n0.975855\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n6\nbpic17-0.3-1\n0.484787\n0.893105\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n7\nwide-0.3-3\n0.926388\n0.847682\n1.000000\n0.946180\n0.995507\n0.000000\n0.000000\n\n\n8\nlarge-0.3-2\n0.951917\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n9\nbpic17-0.3-2\n0.838753\n0.884222\n0.000000\n0.000000\n0.000000\n0.000000\n0.991578\n\n\n10\nbpic13-0.3-1\n0.468346\n0.824835\n0.000000\n0.000000\n0.000000\n0.000000\n0.993919\n\n\n11\nsmall-0.3-3\n0.932209\n0.941794\n0.960505\n0.932191\n0.948540\n0.000000\n0.000000\n\n\n12\nbpic12-0.3-1\n0.717622\n0.975623\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n13\nhuge-0.3-2\n0.904886\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n14\np2p-0.3-3\n0.957556\n0.931714\n0.940759\n1.000000\n1.000000\n0.000000\n0.000000\n\n\n15\nwide-0.3-4\n0.887997\n0.885476\n1.000000\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n16\nhuge-0.3-1\n0.984322\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n17\ngigantic-0.3-1\n0.907391\n0.937300\n0.966925\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n18\np2p-0.3-1\n0.993802\n0.975278\n0.978921\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n19\nwide-0.3-2\n0.958913\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n20\nbpic15-0.3-2\n0.436090\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n21\nlarge-0.3-1\n0.987251\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n22\nbpic15-0.3-1\n0.490875\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n23\ngigantic-0.3-3\n0.936312\n0.895252\n1.000000\n0.976421\n1.000000\n0.000000\n0.000000\n\n\n24\nmedium-0.3-4\n0.931641\n0.940106\n1.000000\n1.000000\n0.984717\n1.000000\n0.000000\n\n\n25\nmedium-0.3-2\n0.935906\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n26\nhuge-0.3-4\n0.887196\n0.896746\n1.000000\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n27\ngigantic-0.3-4\n0.879924\n0.926454\n1.000000\n0.998477\n0.981558\n1.000000\n0.000000\n\n\n28\nbpic13-0.3-2\n0.485372\n0.800000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n29\np2p-0.3-2\n0.975610\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n30\nbpic15-0.3-3\n0.474283\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n31\nbpic15-0.3-5\n0.439946\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n0.818468\n\n\n32\nlarge-0.3-3\n0.892308\n0.867383\n1.000000\n0.978184\n0.996034\n0.000000\n0.000000\n\n\n33\nsmall-0.3-2\n0.973026\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n34\nhuge-0.3-3\n0.824321\n0.960149\n0.938465\n0.862685\n0.993652\n0.000000\n0.000000\n\n\n35\nwide-0.3-1\n0.991540\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n36\npaper-0.3-1\n0.999659\n0.883404\n0.944065\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n37\nsmall-0.3-1\n0.996956\n0.960149\n0.993652\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n38\ngigantic-0.3-2\n0.884754\n0.950029\n0.949633\n0.997772\n0.000000\n0.000000\n0.000000\n\n\n39\np2p-0.3-4\n0.981248\n0.946049\n0.995983\n1.000000\n1.000000\n0.957152\n0.000000\n\n\n\n\n\n\n\n\nanomalie_types = []\n\nfor log_name in progress_bar(logs):\n    score_df,cols,o = get_score_df(log_name)\n    max_error_cases = score_df.groupby('trace_id').max()\n    y_true =o.items.groupby(o.items.index)['anomaly'].last().to_numpy() != 'normal'\n    df =threshold_anomaly_ratio_df_fixed(max_error_cases)\n    h_ths = {col:[i for j in [[k.Th for k in listify(h(df[df['Col']==col]))] for h in heuristics] for i in j] for col in cols}\n    h_ths_df = pd.DataFrame(h_ths,index=heuristic_names).transpose()\n    for hn in heuristic_names:\n        ths =h_ths_df[hn]\n        y_pred =(np.array([(max_error_cases[c]&gt;ths[c]).tolist() for c in cols if c in all_cols]).max(axis=0))   \n        k=o.items.groupby(o.items.index)['anomaly'].last().iloc[np.where(np.equal(y_pred, y_true))[0]]\n        k= k.value_counts().to_dict()\n        k['heuristic']=hn\n        k['log_name'] = log_name\n        k['correct']=True\n        anomalie_types.append(k)\n        k=o.items.groupby(o.items.index)['anomaly'].last().iloc[np.where(~np.equal(y_pred, y_true))[0]]\n        k= k.value_counts().to_dict()\n        k['log_name'] = log_name\n        k['correct']=False\n        k['heuristic']=hn\n        anomalie_types.append(k)\n    settings = best_th_df[best_th_df['log_name']==log_name].iloc[0][2:]\n    y_pred =(np.array([(max_error_cases[c]&gt;settings[c]).tolist() for c in cols if c in all_cols]).max(axis=0))  \n    k=o.items.groupby(o.items.index)['anomaly'].last().iloc[np.where(np.equal(y_pred, y_true))[0]]\n    k= k.value_counts().to_dict()\n    k['heuristic']='best'\n    k['log_name'] = log_name\n    k['correct']=True\n    anomalie_types.append(k)\n    k=o.items.groupby(o.items.index)['anomaly'].last().iloc[np.where(~np.equal(y_pred, y_true))[0]]\n    k= k.value_counts().to_dict()\n    k['log_name'] = log_name\n    k['correct']=False\n    k['heuristic']='best'\n    anomalie_types.append(k)\n\n\n\n\n\n\n    \n      \n      100.00% [40/40 01:03&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_3414969/612902554.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/612902554.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/612902554.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/612902554.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n/tmp/ipykernel_3414969/612902554.py:4: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  score_df,cols,o = get_score_df(log_name)\n/home/lahann/mambaforge/envs/pympp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\nanomalie_types= pd.DataFrame(anomalie_types)\nanomalie_types.fillna(0,inplace=True)\n\n\n1- (anomalie_types[(anomalie_types['heuristic']=='lp_mean')&(anomalie_types['correct']==False)]['Early'].sum() / \nanomalie_types[(anomalie_types['heuristic']=='lp_mean')]['Early'].sum())\n\n0.8302284140612344\n\n\n\nanomalie_types[(anomalie_types['heuristic']=='lp_mean')]['Early'].sum()\n\n12346.0\n\n\n\nanomalie_types_bpi = anomalie_types[anomalie_types['log_name'].str.contains('bpi')]\nanomalie_types_synth = anomalie_types[~anomalie_types['log_name'].str.contains('bpi')]\n\n\nheuristics_desc = ['Best','Elbow↓','Elbow↑','Fix-98','LP-Max','LP-Mean','LP-Min','AR-0.5']\n\n\nanomalie_types = anomalie_types_bpi\nanomalie_types_ratio =(anomalie_types[anomalie_types['correct']==False].groupby(['heuristic'])[anomalie_types.columns[:-3]].sum()\n/ anomalie_types.groupby(['heuristic'])[anomalie_types.columns[:-3]].sum())\nanomalie_types_ratio = 1 - (anomalie_types_ratio)\n\nanomalie_types_ratio['Heuristic']=anomalie_types_ratio.index\nanomalie_types_ratio['Normal']=anomalie_types_ratio['normal']\nanomalie_types_ratio['Heuristic'] = heuristics_desc\nanomalie_types_ratio\n\n\n\n\n\n\n\n\nnormal\nAttribute\nRework\nLate\nInsert\nSkipSequence\nEarly\nHeuristic\nNormal\n\n\nheuristic\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest\n0.645508\n0.676679\n0.974893\n0.915004\n1.000000\n0.946477\n0.927695\nBest\n0.645508\n\n\ne_down\n0.400341\n0.890186\n0.978396\n0.907023\n1.000000\n0.981132\n0.871692\nElbow↓\n0.400341\n\n\ne_up\n0.624262\n0.743447\n0.932269\n0.940942\n0.999808\n0.932807\n0.942846\nElbow↑\n0.624262\n\n\nfix\n0.325507\n0.918883\n0.992799\n0.989824\n1.000000\n0.994417\n0.992520\nFix-98\n0.325507\n\n\nlp_max\n0.479932\n0.854601\n0.821331\n0.726457\n0.999424\n0.931267\n0.738013\nLP-Max\n0.479932\n\n\nlp_mean\n0.476992\n0.854792\n0.823472\n0.727454\n0.999424\n0.932807\n0.739164\nLP-Mean\n0.476992\n\n\nlp_min\n0.476826\n0.855366\n0.824835\n0.728053\n0.999424\n0.933577\n0.739356\nLP-Min\n0.476826\n\n\nr_0.5\n0.528791\n0.877559\n0.958155\n0.956504\n1.000000\n0.955333\n0.963368\nAR-0.5\n0.528791\n\n\n\n\n\n\n\n\ncolor_palette = sns.color_palette()\ndf = anomalie_types_ratio.melt(var_name='Anomaly Type',value_name='Precision',id_vars ='Heuristic',value_vars=['Normal',\n 'Rework',\n 'Late',\n 'Insert',\n 'SkipSequence',\n 'Attribute',\n 'Early'],ignore_index=False)\ng = sns.catplot(x=\"Anomaly Type\", y=\"Precision\",palette=color_palette,\n                data=df, kind=\"bar\",hue='Heuristic',\n height=3.5, aspect=3.7);\n\n\n\n\n\nanomalie_types = anomalie_types_synth\nanomalie_types_ratio =(anomalie_types[anomalie_types['correct']==False].groupby(['heuristic'])[anomalie_types.columns[:-3]].sum()\n/ anomalie_types.groupby(['heuristic'])[anomalie_types.columns[:-3]].sum())\nanomalie_types_ratio = 1 - (anomalie_types_ratio)\n\nanomalie_types_ratio['Heuristic']=anomalie_types_ratio.index\nanomalie_types_ratio['Normal']=anomalie_types_ratio['normal']\nanomalie_types_ratio['Heuristic'] = heuristics_desc\nanomalie_types_ratio\n\n\n\n\n\n\n\n\nnormal\nAttribute\nRework\nLate\nInsert\nSkipSequence\nEarly\nHeuristic\nNormal\n\n\nheuristic\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest\n0.962247\n0.789085\n1.000000\n1.000000\n1.0\n0.999312\n1.000000\nBest\n0.962247\n\n\ne_down\n0.880865\n0.974662\n0.996899\n0.924728\n1.0\n0.974401\n0.938446\nElbow↓\n0.880865\n\n\ne_up\n0.881101\n0.933872\n0.991683\n0.977295\n1.0\n0.972887\n0.985558\nElbow↑\n0.881101\n\n\nfix\n0.778484\n0.987053\n1.000000\n1.000000\n1.0\n1.000000\n1.000000\nFix-98\n0.778484\n\n\nlp_max\n0.966109\n0.960880\n0.983225\n0.868722\n1.0\n0.917148\n0.896663\nLP-Max\n0.966109\n\n\nlp_mean\n0.965834\n0.960880\n0.983225\n0.868722\n1.0\n0.918112\n0.896803\nLP-Mean\n0.965834\n\n\nlp_min\n0.965637\n0.960880\n0.983225\n0.868997\n1.0\n0.918525\n0.896803\nLP-Min\n0.965637\n\n\nr_0.5\n0.778317\n0.987053\n1.000000\n1.000000\n1.0\n1.000000\n1.000000\nAR-0.5\n0.778317\n\n\n\n\n\n\n\n\ncolor_palette = sns.color_palette()\ndf = anomalie_types_ratio.melt(var_name='Anomaly Type',value_name='Precision',id_vars ='Heuristic',value_vars=['Normal',\n 'Rework',\n 'Late',\n 'Insert',\n 'SkipSequence',\n 'Attribute',\n 'Early'],ignore_index=False)\ng = sns.catplot(x=\"Anomaly Type\", y=\"Precision\",palette=color_palette,\n                data=df, kind=\"bar\",hue='Heuristic',\n                height=3.5, aspect=3.7);"
  },
  {
    "objectID": "06_anomaly.detect.html",
    "href": "06_anomaly.detect.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Includes the anomaly detection algorithm, i.e. the prediction model, the loss functions and the anomaly score calculation and classification, as well as the metric computation."
  },
  {
    "objectID": "06_anomaly.detect.html#only-control-flow",
    "href": "06_anomaly.detect.html#only-control-flow",
    "title": "Anomaly Detection",
    "section": "Only Control-flow",
    "text": "Only Control-flow\n\nlog_name='pdc_2020_0101100'\n\n\nlog = import_log(f'./data/logs/csv/dapnn_ds/PDC2020_training/{log_name}.csv.gz')\nlog.head(20)\n\n\n\n\n\n\n\n\nactivity\nevent_id\n\n\ntrace_id\n\n\n\n\n\n\ntrace 1\n###start###\n0\n\n\ntrace 1\nt31\n1\n\n\ntrace 1\nt41\n2\n\n\ntrace 1\nt26\n3\n\n\ntrace 1\nt36\n4\n\n\ntrace 1\nt44\n5\n\n\ntrace 1\nt54\n6\n\n\ntrace 1\nt33\n7\n\n\ntrace 1\nt23\n8\n\n\ntrace 1\nt65\n9\n\n\ntrace 1\nt76\n10\n\n\ntrace 1\nt21\n11\n\n\ntrace 1\nt74\n12\n\n\ntrace 1\nt32\n13\n\n\ntrace 1\nt64\n14\n\n\ntrace 1\nt41\n15\n\n\ntrace 1\nt33\n16\n\n\ntrace 1\nt23\n17\n\n\ntrace 1\nt21\n18\n\n\ntrace 1\nt32\n19\n\n\n\n\n\n\n\nCreate PPObj with vocab and dataloaders\n\nsource\n\ntraining_dl\n\n training_dl (log, cat_names='activity', seed=45, ws=5, bs=32)\n\n\no,dls,categorify = training_dl(log)\n\n\nx,y= dls.one_batch()\n\nDefine Deep Learning Model\n\nsource\n\n\nControlFlowModel\n\n ControlFlowModel (o)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nm=ControlFlowModel(o)\n\n\npred=m(x.cpu())\n\n\ny[0].cpu()\n\ntensor([20, 14, 20, 24, 10, 19, 23, 18, 23, 14, 14, 15, 14,  6, 19, 12, 14,  8,\n        23, 16, 17, 10, 22, 13,  6, 26, 13, 14, 11, 19,  6, 15])\n\n\n\nsqueeze_cross_entropy = lambda x,y:F.cross_entropy(x,y[0])\nsqueeze_accuracy =lambda x,y:accuracy(x,y[0])\n\n\nsqueeze_cross_entropy(pred.cuda(),y),squeeze_accuracy(pred.cuda(),y)\n\n(tensor(3.2628, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;),\n TensorBase(0.1875, device='cuda:0'))\n\n\nDefine Training Loop and Helper Functions\n\nsource\n\n\ntrain_validate\n\n train_validate (dls, m, metrics=&lt;function accuracy&gt;, loss=&lt;function\n                 cross_entropy&gt;, epoch=20, print_output=True,\n                 model_dir='.', lr_find=True, patience=5, min_delta=0.005,\n                 show_plot=True, store_path='tmp', model_name='.model')\n\nTrains a model on the training set with early stopping based on the validation loss. Afterwards, applies it to the test set.\n\nsource\n\n\ntraining_loop\n\n training_loop (learn, epoch, print_output, lr_find)\n\nBasic training loop that uses learning rate finder and one cycle training. See fastai docs for more information\n\nsource\n\n\nHideOutput\n\n HideOutput ()\n\nA utility function that hides all outputs in a context\nTrain prediction model\n\ntrain_val = train_validate(dls,m,epoch=25,metrics=squeeze_accuracy,loss=squeeze_cross_entropy)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\n\ntime\n\n\n\n\n0\n2.517840\n2.400468\n0.266471\n00:06\n\n\n1\n1.628425\n1.553959\n0.476818\n00:06\n\n\n2\n1.312923\n1.262455\n0.503660\n00:06\n\n\n3\n1.166748\n1.173147\n0.507077\n00:06\n\n\n4\n1.141692\n1.086159\n0.520742\n00:06\n\n\n5\n1.081097\n1.092973\n0.495364\n00:06\n\n\n6\n1.072851\n1.053992\n0.523670\n00:06\n\n\n7\n1.057887\n1.058307\n0.514397\n00:06\n\n\n8\n1.058676\n1.028091\n0.517326\n00:06\n\n\n9\n1.012709\n1.033957\n0.514397\n00:06\n\n\n10\n1.028024\n1.039211\n0.518302\n00:06\n\n\n11\n1.017703\n1.024853\n0.518302\n00:05\n\n\n12\n1.000363\n1.023521\n0.523670\n00:06\n\n\n13\n1.007020\n1.011746\n0.515861\n00:06\n\n\n14\n0.977153\n1.010085\n0.525622\n00:06\n\n\n15\n0.951577\n1.014484\n0.525134\n00:06\n\n\n16\n0.982534\n1.011046\n0.526110\n00:06\n\n\n17\n0.947973\n1.008882\n0.526110\n00:06\n\n\n18\n0.965707\n1.007773\n0.523182\n00:06\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 2.400468349456787.\nBetter model found at epoch 1 with valid_loss value: 1.5539591312408447.\nBetter model found at epoch 2 with valid_loss value: 1.262454628944397.\nBetter model found at epoch 3 with valid_loss value: 1.173147201538086.\nBetter model found at epoch 4 with valid_loss value: 1.0861592292785645.\nBetter model found at epoch 6 with valid_loss value: 1.0539923906326294.\nBetter model found at epoch 8 with valid_loss value: 1.028091311454773.\nBetter model found at epoch 11 with valid_loss value: 1.0248527526855469.\nBetter model found at epoch 12 with valid_loss value: 1.0235213041305542.\nBetter model found at epoch 13 with valid_loss value: 1.0117459297180176.\nBetter model found at epoch 14 with valid_loss value: 1.010084867477417.\nBetter model found at epoch 17 with valid_loss value: 1.0088822841644287.\nBetter model found at epoch 18 with valid_loss value: 1.0077729225158691.\nNo improvement since epoch 13: early stopping\nBetter model found at epoch 0 with valid_loss value: 0.5207814574241638.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoad Test Log\n\nsource\n\n\nimport_log_pdc\n\n import_log_pdc (log)\n\n\ntest_log = import_log_pdc(f'../data/logs/csv/dapnn_ds/PDC2020_ground_truth/{log_name}.csv.gz')\n\ntest_log.head(25)\n\n\n\n\n\n\n\n\nactivity\ncase:pdc:costs\nnormal\nevent_id\n\n\ntrace_id\n\n\n\n\n\n\n\n\ntrace 1\n###start###\n0.0\nTrue\n0\n\n\ntrace 1\nt21\n0.0\nTrue\n1\n\n\ntrace 1\nt32\n0.0\nTrue\n2\n\n\ntrace 1\nt26\n0.0\nTrue\n3\n\n\ntrace 1\nt41\n0.0\nTrue\n4\n\n\ntrace 1\nt35\n0.0\nTrue\n5\n\n\ntrace 1\nt44\n0.0\nTrue\n6\n\n\ntrace 1\nt34\n0.0\nTrue\n7\n\n\ntrace 1\nt24\n0.0\nTrue\n8\n\n\ntrace 1\nt33\n0.0\nTrue\n9\n\n\ntrace 1\nt26\n0.0\nTrue\n10\n\n\ntrace 1\nt36\n0.0\nTrue\n11\n\n\ntrace 1\nt44\n0.0\nTrue\n12\n\n\ntrace 1\nt23\n0.0\nTrue\n13\n\n\ntrace 1\nt54\n0.0\nTrue\n14\n\n\ntrace 1\nt21\n0.0\nTrue\n15\n\n\ntrace 1\nt32\n0.0\nTrue\n16\n\n\ntrace 1\nt66\n0.0\nTrue\n17\n\n\ntrace 1\nt41\n0.0\nTrue\n18\n\n\ntrace 1\nt51\n0.0\nTrue\n19\n\n\ntrace 1\nt62\n0.0\nTrue\n20\n\n\ntrace 1\nt71\n0.0\nTrue\n21\n\n\ntrace 1\nt76\n0.0\nTrue\n22\n\n\ntrace 1\n###end###\n0.0\nTrue\n23\n\n\ntrace 2\n###start###\n0.0\nTrue\n0\n\n\n\n\n\n\n\nCreate PPOBJ of test data with same vocab\n\nsource\n\n\nprocess_test\n\n process_test (test_log, categorify, cat_names='activity')\n\n\no = process_test(test_log,categorify)\n\nApply Prediction Model\n\nsource\n\n\npredict_next_step\n\n predict_next_step (o, m, ws=5)\n\n\nnsp,idx=predict_next_step(o,m)\n\n\nnsp.shape\n\ntorch.Size([29095, 27])\n\n\nCalculate Anomaly Score\n\nsource\n\n\ncalc_anomaly_score\n\n calc_anomaly_score (res, o, idx)\n\n\nanomaly_score = calc_anomaly_score(nsp,o,idx)\n\nClassify Anomalies based of Threshhold, get predictions and ground truth, compute a few metrics\n\nsource\n\n\nget_anomalies\n\n get_anomalies (a_score, o, idx, threshhold=0.98)\n\n\ny_pred, y_true = get_anomalies(anomaly_score,o,idx)\n\n\nf1_score(y_true, y_pred)\n\n0.8446139180171591\n\n\n\naccuracy_score(y_true, y_pred)\n\n0.828\n\n\n\nprecision_score(y_true,y_pred)\n\n0.7289256198347107\n\n\n\nrecall_score(y_true,y_pred)\n\n0.9821826280623608\n\n\nGet NSP Accuracy on Inrference\n\nsource\n\n\nnsp_accuracy\n\n nsp_accuracy (o, idx, nsp)\n\n\nnsp_accuracy(o,idx,nsp)\n\nTensorBase(0.5081, device='cuda:0')\n\n\nRewrite model to support multivariate prediction. Define attr_dict that maps event log to considerable attributes. Dynamically adapt preprocessing, network architecture, loss function, and anomaly score computation"
  },
  {
    "objectID": "06_anomaly.detect.html#multivariate-anomaly-detection",
    "href": "06_anomaly.detect.html#multivariate-anomaly-detection",
    "title": "Anomaly Detection",
    "section": "Multivariate Anomaly Detection",
    "text": "Multivariate Anomaly Detection\n\nattr_dict\n\n{'bpic15-0.3-4': ['activity',\n  'action_code',\n  'activityNameEN',\n  'activityNameNL',\n  'monitoringResource',\n  'org:resource',\n  'question'],\n 'large-0.3-4': ['activity', 'user', 'day', 'country', 'company'],\n 'small-0.3-4': ['activity', 'user', 'day', 'country', 'company'],\n 'bpic13-0.3-3': ['activity',\n  'org:group',\n  'org:resource',\n  'org:role',\n  'organization country',\n  'product',\n  'resource country',\n  'impact'],\n 'medium-0.3-3': ['activity', 'user', 'day', 'country'],\n 'medium-0.3-1': ['activity', 'user'],\n 'bpic17-0.3-1': ['activity', 'EventOrigin', 'org:resource'],\n 'wide-0.3-3': ['activity', 'user', 'day', 'country'],\n 'large-0.3-2': ['activity', 'user', 'day'],\n 'bpic17-0.3-2': ['activity', 'EventOrigin', 'org:resource'],\n 'bpic13-0.3-1': ['activity',\n  'org:group',\n  'org:resource',\n  'org:role',\n  'organization country',\n  'product',\n  'resource country',\n  'impact'],\n 'small-0.3-3': ['activity', 'user', 'day', 'country'],\n 'bpic12-0.3-1': ['activity'],\n 'huge-0.3-2': ['activity', 'user', 'day'],\n 'p2p-0.3-3': ['activity', 'user', 'day', 'country'],\n 'wide-0.3-4': ['activity', 'user', 'day', 'country', 'company'],\n 'huge-0.3-1': ['activity', 'user'],\n 'gigantic-0.3-1': ['activity', 'user'],\n 'p2p-0.3-1': ['activity', 'user'],\n 'wide-0.3-2': ['activity', 'user', 'day'],\n 'bpic15-0.3-2': ['activity',\n  'action_code',\n  'activityNameEN',\n  'activityNameNL',\n  'monitoringResource',\n  'org:resource',\n  'question'],\n 'large-0.3-1': ['activity', 'user'],\n 'bpic15-0.3-1': ['activity',\n  'action_code',\n  'activityNameEN',\n  'activityNameNL',\n  'monitoringResource',\n  'org:resource',\n  'question'],\n 'gigantic-0.3-3': ['activity', 'user', 'day', 'country'],\n 'medium-0.3-4': ['activity', 'user', 'day', 'country', 'company'],\n 'medium-0.3-2': ['activity', 'user', 'day'],\n 'huge-0.3-4': ['activity', 'user', 'day', 'country', 'company'],\n 'gigantic-0.3-4': ['activity', 'user', 'day', 'country', 'company'],\n 'bpic13-0.3-2': ['activity',\n  'org:group',\n  'org:resource',\n  'org:role',\n  'organization country',\n  'product',\n  'resource country',\n  'impact'],\n 'p2p-0.3-2': ['activity', 'user', 'day'],\n 'bpic15-0.3-3': ['activity',\n  'action_code',\n  'activityNameEN',\n  'activityNameNL',\n  'monitoringResource',\n  'org:resource',\n  'question'],\n 'bpic15-0.3-5': ['activity',\n  'action_code',\n  'activityNameEN',\n  'activityNameNL',\n  'monitoringResource',\n  'org:resource',\n  'question'],\n 'large-0.3-3': ['activity', 'user', 'day', 'country'],\n 'small-0.3-2': ['activity', 'user', 'day'],\n 'huge-0.3-3': ['activity', 'user', 'day', 'country'],\n 'wide-0.3-1': ['activity', 'user'],\n 'paper-0.3-1': ['activity', 'user'],\n 'small-0.3-1': ['activity', 'user'],\n 'gigantic-0.3-2': ['activity', 'user', 'day'],\n 'p2p-0.3-4': ['activity', 'user', 'day', 'country', 'company']}\n\n\n\nsource\n\nget_attr\n\n get_attr (attr_dict, log_name)\n\n\nlog_name= 'small-0.3-4'\nfn=f'../data/logs/csv/dapnn_ds/binet_logs/{log_name}.csv.gz'\ncols= get_attr(attr_dict,log_name)\ncols\n\n['activity', 'user', 'day', 'country', 'company']\n\n\n\nlog = import_log(fn,cols)\nlog.head()\n\n\n\n\n\n\n\n\nactivity\ntimestamp\ntimestamp_end\nanomaly\ncompany\ncountry\nday\nuser\nevent_id\n\n\ntrace_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n###start###\nNaN\nNaN\nnormal\n###start###\n###start###\n###start###\n###start###\n0\n\n\n1\nActivity A\nNaN\nNaN\nnormal\nToughzap\nPeru\nMonday\nSonia\n1\n\n\n1\nActivity B\nNaN\nNaN\nnormal\nRundofase\nIsrael\nMonday\nJack\n2\n\n\n1\nActivity C\nNaN\nNaN\nnormal\nStanredtax\nSaint Lucia\nMonday\nEarl\n3\n\n\n1\nActivity D\nNaN\nNaN\nnormal\nCondax\nGhana\nMonday\nMaryellen\n4\n\n\n\n\n\n\n\n\no,dls,categorify = training_dl(log,cols)\n\n\nxcat,ycat = dls.one_batch()\nxcat.shape,ycat[0]\n\n(torch.Size([32, 5, 5]),\n tensor([10,  3,  9, 12, 18, 18,  9,  3,  3,  1,  1,  7, 11,  1,  1,  1,  5,  6,\n          1,  5,  8,  4, 17, 18,  3,  3,  4,  4, 13,  6, 21, 12],\n        device='cuda:0'))\n\n\n\nemb_szs = get_emb_sz(o)\n\n\nfn=f'../data/logs/csv/dapnn_ds/binet_logs/{log_name}.csv.gz'\n\n\nsource\n\n\nMultivariateModel\n\n MultivariateModel (emb_szs, lstm_neurons=25, lstm_layers=2)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nm=MultivariateModel(emb_szs)\npred = m(xcat.cpu())\n[i.shape for i in pred]\n\n[torch.Size([32, 42]),\n torch.Size([32, 138]),\n torch.Size([32, 14]),\n torch.Size([32, 295]),\n torch.Size([32, 142])]\n\n\n\nsource\n\n\nmy_metric\n\n my_metric (p, y)\n\n\nsource\n\n\nmy_loss\n\n my_loss (p, y)\n\n\nsource\n\n\nmulti_loss_sum\n\n multi_loss_sum (o, p, y)\n\nMulti Loss function that sums up multiple loss functions. The selection of the loss function is based on the PPObj o\n\nsource\n\n\nget_metrics\n\n get_metrics (o)\n\n\nloss=partial(multi_loss_sum,o)\n\n\nloss(pred,[i.cpu() for i in ycat])\n\ntensor(21.9264, grad_fn=&lt;SumBackward0&gt;)\n\n\n\nepoch=1\ntrain_val=train_validate(dls,m,loss=loss,metrics=get_metrics(o),epoch=epoch,show_plot=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\n_accuracy_idx\n_accuracy_idx\n_accuracy_idx\n_accuracy_idx\n_accuracy_idx\ntime\n\n\n\n\n0\n7.136340\n7.045480\n0.826224\n0.544595\n0.477460\n0.553563\n0.659477\n00:32\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 7.045479774475098.\nBetter model found at epoch 0 with valid_loss value: 0.8068852424621582.\n\n\n\n\n\n\n\n\n\n\n\nLoad Test Log for Inference\n\nlog = import_log(fn)\nlog.head()\no = process_test(log,categorify,cols)\n\n\nsource\n\n\npredict_next_step\n\n predict_next_step (o, m, ws=5)\n\n\nres,idx=predict_next_step(o,m)\n\nCPU times: user 1.18 s, sys: 183 ms, total: 1.36 s\nWall time: 315 ms\n\n\n\nsource\n\n\nmultivariate_anomaly_score\n\n multivariate_anomaly_score (res, o, idx, cols)\n\n\nscore_df=multivariate_anomaly_score(res,o,idx,cols)\nscore_df\n\n\n\n\n\n\n\n\nactivity\nuser\nday\ncountry\ncompany\ntrace_id\n\n\n\n\n0\n0.000000\n0.439668\n0.000000\n0.531000\n0.315139\n1\n\n\n1\n0.000000\n0.662233\n0.000000\n0.000000\n0.000000\n1\n\n\n2\n0.000000\n0.380046\n0.000000\n0.532485\n0.000000\n1\n\n\n3\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1\n\n\n4\n0.000000\n0.904819\n0.362798\n0.681615\n0.828734\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n51690\n0.000000\n0.720104\n0.537395\n0.000000\n0.000000\n5000\n\n\n51691\n0.000000\n0.000000\n0.298893\n0.097286\n0.000000\n5000\n\n\n51692\n0.989942\n0.946027\n0.391907\n0.980420\n0.982797\n5000\n\n\n51693\n0.000000\n0.345933\n0.000000\n0.867805\n0.635300\n5000\n\n\n51694\n0.000000\n0.909175\n0.089738\n0.985204\n0.862531\n5000\n\n\n\n\n51695 rows × 6 columns\n\n\n\n\n\nAnomaly Classification\nWe can set a fixed threshold or define a custom threshold function. Currently only fixed threshold is used.\n\nsource\n\n\nget_thresholds\n\n get_thresholds (col, act_threshold=0.964, attr_threshold=0.9971)\n\nDefines a custom threshold function\n\nsource\n\n\nmultivariate_anomalies\n\n multivariate_anomalies (score_df, cols, idx, o, anomaly_col='normal',\n                         fixed_threshold=None, get_thresholds=&lt;function\n                         get_thresholds&gt;)\n\n\ny_true,y_pred=multivariate_anomalies(score_df,cols,idx,o,fixed_threshold=None,anomaly_col='anomaly')\n\n\nf1_score(y_true, y_pred)\n\n0.851963746223565\n\n\n\nk=o.items.groupby(o.items.index)['anomaly'].last().iloc[np.where(~np.equal(y_pred, y_true))[0]\n]\nk.value_counts()\n\nnormal          371\nSkipSequence     51\nAttribute        32\nEarly            18\nLate             18\nName: anomaly, dtype: int64\n\n\n\no.items.groupby(o.items.index)['anomaly'].last().value_counts()\n\nnormal          3471\nRework           274\nSkipSequence     268\nLate             261\nEarly            253\nAttribute        239\nInsert           234\nName: anomaly, dtype: int64"
  },
  {
    "objectID": "06_anomaly.detect.html#plot-anomaly-scores-for-a-trace",
    "href": "06_anomaly.detect.html#plot-anomaly-scores-for-a-trace",
    "title": "Anomaly Detection",
    "section": "Plot Anomaly Scores for a Trace",
    "text": "Plot Anomaly Scores for a Trace\n\nscore_df['event_id']=score_df.groupby('trace_id').cumcount()\n\nplot_data=score_df.melt(value_vars=['activity','user','day','country','company'],value_name='Anomaly Score',var_name='Attribute',id_vars=['trace_id','anomaly','event_id'])\nplot_data['event_id']=plot_data.groupby('trace_id').cumcount()\nplot_data\n\n\n\n\n\n\n\n\ntrace_id\nanomaly\nevent_id\nAttribute\nAnomaly Score\n\n\n\n\n0\n1\nnormal\n0\nactivity\n0.000000\n\n\n1\n1\nnormal\n1\nactivity\n0.000000\n\n\n2\n1\nnormal\n2\nactivity\n0.000000\n\n\n3\n1\nnormal\n3\nactivity\n0.000000\n\n\n4\n1\nnormal\n4\nactivity\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n\n\n258470\n5000\nSkipSequence\n40\ncompany\n0.000000\n\n\n258471\n5000\nSkipSequence\n41\ncompany\n0.000000\n\n\n258472\n5000\nSkipSequence\n42\ncompany\n0.982797\n\n\n258473\n5000\nSkipSequence\n43\ncompany\n0.635300\n\n\n258474\n5000\nSkipSequence\n44\ncompany\n0.862531\n\n\n\n\n258475 rows × 5 columns\n\n\n\n\nimport seaborn as sns\n\n\ndata=plot_data[plot_data['trace_id']==6].copy()\ndata['xticks']= [f'e{y+1}' for x in range(5) for y in range(len(data)//5)]\nprint(len(data))\nprint(data['anomaly'].iloc[0])\nprint(data['trace_id'].iloc[0])\ntimes = data.event_id.unique()\ng = sns.FacetGrid(data, col=\"trace_id\", hue=\"Attribute\",\n                  palette=\"Set3\", height=4, aspect=2)\ng.map(sns.barplot, 'event_id', 'Anomaly Score', order=times)\ng.add_legend()\nax1 = g.axes[0][0]\ng.set_titles(col_template=\"\", row_template=\"\")\ng.set_axis_labels('')\n\nax1.axhline(0.98, ls='--')\nfor ax in g.axes.flat:\n  \n    ax.set_xticklabels([f'e{y+1}' for x in range(5) for y in range(len(data)//5)], rotation=40,fontsize=9) # set new labels\nplt.show()\n\n45\nSkipSequence\n6\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n\ndef plot_anomaly_scores(trace_id):\n    data=plot_data[plot_data['trace_id']==trace_id]\n    print(len(data))\n    print(data['anomaly'].iloc[0])\n    print(data['trace_id'].iloc[0])\n    times = data.event_id.unique()\n    g = sns.FacetGrid(data, col=\"trace_id\", hue=\"Attribute\",\n                      palette=\"Set3\", height=4, aspect=2)\n    g.map(sns.barplot, 'event_id', 'Anomaly Score', order=times)\n    g.add_legend()\n    ax1 = g.axes[0][0]\n    g.set_titles(col_template=\"\", row_template=\"\")\n    g.set_axis_labels('')\n    ax1.axhline(0.98, ls='--')\n    for ax in g.axes.flat:\n\n        ax.set_xticklabels([f'e{y+1}' for x in range(5) for y in range(len(data)//5)], rotation=50,fontsize=11) # set new labels\n    plt.show()\n\n\nplot_anomaly_scores(1)\n\n40\nnormal\n1\n\n\n\n\n\n\nplot_anomaly_scores(6)\n\n45\nSkipSequence\n6"
  },
  {
    "objectID": "00_logs.html",
    "href": "00_logs.html",
    "title": "Logs",
    "section": "",
    "text": "Convert xes event logs to csv format"
  },
  {
    "objectID": "00_logs.html#convert-to-pandas-dataframe",
    "href": "00_logs.html#convert-to-pandas-dataframe",
    "title": "Logs",
    "section": "Convert to pandas dataframe",
    "text": "Convert to pandas dataframe\n\nsource\n\nconvert_to_df\n\n convert_to_df (path)\n\n\nbpi_2012 = \"../data/logs/xes/BPI_Challenge_2012.xes.gz\"\nconvert_to_df(bpi_2012)\n\n\n\n\n\n\n\n\n\n\n\norg:resource\nlifecycle:transition\nconcept:name\ntime:timestamp\ncase:REG_DATE\ncase:concept:name\ncase:AMOUNT_REQ\n\n\n\n\n0\n112\nCOMPLETE\nA_SUBMITTED\n2011-09-30 22:38:44.546000+00:00\n2011-09-30 22:38:44.546000+00:00\n173688\n20000\n\n\n1\n112\nCOMPLETE\nA_PARTLYSUBMITTED\n2011-09-30 22:38:44.880000+00:00\n2011-09-30 22:38:44.546000+00:00\n173688\n20000\n\n\n2\n112\nCOMPLETE\nA_PREACCEPTED\n2011-09-30 22:39:37.906000+00:00\n2011-09-30 22:38:44.546000+00:00\n173688\n20000\n\n\n3\n112\nSCHEDULE\nW_Completeren aanvraag\n2011-09-30 22:39:38.875000+00:00\n2011-09-30 22:38:44.546000+00:00\n173688\n20000\n\n\n4\nNaN\nSTART\nW_Completeren aanvraag\n2011-10-01 09:36:46.437000+00:00\n2011-09-30 22:38:44.546000+00:00\n173688\n20000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n262195\n112\nCOMPLETE\nA_PARTLYSUBMITTED\n2012-02-29 22:51:17.423000+00:00\n2012-02-29 22:51:16.799000+00:00\n214376\n15000\n\n\n262196\n112\nSCHEDULE\nW_Afhandelen leads\n2012-02-29 22:52:01.287000+00:00\n2012-02-29 22:51:16.799000+00:00\n214376\n15000\n\n\n262197\n11169\nSTART\nW_Afhandelen leads\n2012-03-01 08:26:46.736000+00:00\n2012-02-29 22:51:16.799000+00:00\n214376\n15000\n\n\n262198\n11169\nCOMPLETE\nA_DECLINED\n2012-03-01 08:27:37.118000+00:00\n2012-02-29 22:51:16.799000+00:00\n214376\n15000\n\n\n262199\n11169\nCOMPLETE\nW_Afhandelen leads\n2012-03-01 08:27:41.325000+00:00\n2012-02-29 22:51:16.799000+00:00\n214376\n15000\n\n\n\n\n262200 rows × 7 columns"
  },
  {
    "objectID": "00_logs.html#sample-conversion-for-pdc-and-binet-logs",
    "href": "00_logs.html#sample-conversion-for-pdc-and-binet-logs",
    "title": "Logs",
    "section": "Sample Conversion for PDC and BINET Logs",
    "text": "Sample Conversion for PDC and BINET Logs\n\nPDC 2020\n\nTraining Logs\n\nfor fn in progress_bar(glob.glob('data/orig/unzipped/PDC2020_training/*')):\n    new_fn=fn.split('.')[0]+'.csv.gz'\n    new_fn='data/csv/'+\"/\".join(new_fn.split('/')[3:])\n    log = xes_importer.apply(fn)\n    df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n    df.to_csv(new_fn,index=False,compression='gzip')\n\n\n\nGroundtruth Logs\n\nfor fn in progress_bar(glob.glob('data/orig/unzipped/PDC2020_ground_truth/*')):\n    new_fn=fn.split('.')[0]+'.csv.gz'\n    new_fn='data/csv/'+\"/\".join(new_fn.split('/')[3:])\n    log = xes_importer.apply(fn)\n    df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n    df.to_csv(new_fn,index=False,compression='gzip')\n\n\n\n\nPDC 2021\n\nTraining Logs\n\nfor fn in progress_bar(glob.glob('data/orig/unzipped/PDC2021_training/*')):\n    new_fn=fn.split('.')[0]+'.csv.gz'\n    new_fn='data/csv/'+\"/\".join(new_fn.split('/')[3:])\n    log = xes_importer.apply(fn)\n    df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n    df.to_csv(new_fn,index=False,compression='gzip')\n\n\n\nGroundtruth Logs\n\nfor fn in progress_bar(glob.glob('data/orig/unzipped/PDC2021_ground_truth/*')):\n    new_fn=fn.split('.')[0]+'.csv.gz'\n    new_fn='data/csv/'+\"/\".join(new_fn.split('/')[3:])\n    log = xes_importer.apply(fn)\n    df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n    df.to_csv(new_fn,index=False,compression='gzip')\n\n\n\n\nBINET Logs\n\ndef load_binet_data (path):\n    with gzip.open(path, \"r\") as f:\n        data = f.read()\n        j = json.loads (data.decode('utf-8'))\n        \n    event_df = pd.DataFrame({'attributes': [], 'name': [], 'timestamp': [],'timestamp_end': [], 'event_id': [], 'trace_id': []})\n    truth_df = pd.DataFrame({'case': [], 'anomalies': []})\n    res=[]\n    res=pd.DataFrame()\n    for case in (j['cases']):\n        trace = pd.DataFrame.from_dict(case['events'])\n        trace['anomaly']= case['attributes']['label'] if type(case['attributes']['label'])==str else case['attributes']['label']['anomaly']\n        trace['trace_id']=case['id']\n        res=res.append(trace)\n    res=pd.concat([res.drop(['attributes'], axis=1), res['attributes'].apply(pd.Series)], axis=1)\n    return res\n\n\nfor fn in progress_bar((list(glob.glob('data/orig/unzipped/binet_logs/*-0.3-*')))):\n    new_fn=fn[:-7]+'csv.gz'\n    new_fn='data/csv/'+\"/\".join(new_fn.split('/')[3:])\n    df=load_binet_data(fn)\n    df.to_csv(new_fn,index=False,compression='gzip')"
  },
  {
    "objectID": "08_anomaly.heuristics.html",
    "href": "08_anomaly.heuristics.html",
    "title": "Anomaly Detection Heuristics",
    "section": "",
    "text": "This notebook reimplements and prints different heuristics from Nolle 2019 including - elbow_up - elbow_down - lowest_plateau_left - lowest_plateau_right - lowest_plateau_centered - best"
  },
  {
    "objectID": "08_anomaly.heuristics.html#get-anomaly-scores",
    "href": "08_anomaly.heuristics.html#get-anomaly-scores",
    "title": "Anomaly Detection Heuristics",
    "section": "Get Anomaly Scores:",
    "text": "Get Anomaly Scores:\n\nsource\n\nf_score\n\n f_score (y_true, y_pred, sample_weight=None)\n\n\nsource\n\n\nload_pred_model\n\n load_pred_model (learner_path, train_log_path, log_name,\n                  cols=['activity'])\n\n\nsource\n\n\nmultivariate_anomaly_score\n\n multivariate_anomaly_score (res, o, idx, cols, normalization=True)\n\n\nsource\n\n\nget_score_df\n\n get_score_df (log_name, prediction_normalization=True)\n\n\nlog_name = '1001001'\nscore_df, y_true = get_score_df(log_name)\nscore_df\n\n\n\n\n\n\n\n\nactivity\ntrace_id\n\n\n\n\n0\n0.000000\ntrace 1\n\n\n1\n0.000000\ntrace 1\n\n\n2\n0.052144\ntrace 1\n\n\n3\n0.000000\ntrace 1\n\n\n4\n0.121892\ntrace 1\n\n\n...\n...\n...\n\n\n12992\n0.000000\ntrace 1000\n\n\n12993\n0.000000\ntrace 1000\n\n\n12994\n0.000000\ntrace 1000\n\n\n12995\n0.000000\ntrace 1000\n\n\n12996\n0.000000\ntrace 1000\n\n\n\n\n12997 rows × 2 columns"
  },
  {
    "objectID": "08_anomaly.heuristics.html#get-predictions-per-threshold",
    "href": "08_anomaly.heuristics.html#get-predictions-per-threshold",
    "title": "Anomaly Detection Heuristics",
    "section": "Get Predictions per threshold",
    "text": "Get Predictions per threshold\n\nsource\n\nget_preds\n\n get_preds (score_df, threshold)\n\n\nths = np.array((range(1000)))*0.001 +0.0\nths[:20]\n\narray([0.   , 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008,\n       0.009, 0.01 , 0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017,\n       0.018, 0.019])\n\n\n\nsource\n\n\nget_th_df\n\n get_th_df (ths, score_df, y_true, log_name, f_score=&lt;function f_score&gt;)\n\n\nth_df = get_th_df(ths,score_df,y_true,log_name)\nth_df\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:06&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nLog Name\nF1 Score\nPrecision\nRecall\nAnomaly Ratio\n\n\n\n\n0.000\n1001001\n0.003914\n0.489980\n0.997959\n0.998\n\n\n0.001\n1001001\n0.003914\n0.489980\n0.997959\n0.998\n\n\n0.002\n1001001\n0.003914\n0.489980\n0.997959\n0.998\n\n\n0.003\n1001001\n0.003914\n0.489980\n0.997959\n0.998\n\n\n0.004\n1001001\n0.003914\n0.489980\n0.997959\n0.998\n\n\n...\n...\n...\n...\n...\n...\n\n\n0.995\n1001001\n0.775937\n0.996795\n0.634694\n0.312\n\n\n0.996\n1001001\n0.743047\n0.996564\n0.591837\n0.291\n\n\n0.997\n1001001\n0.716772\n0.996364\n0.559184\n0.275\n\n\n0.998\n1001001\n0.657110\n0.995851\n0.489796\n0.241\n\n\n0.999\n1001001\n0.545701\n0.994595\n0.375510\n0.185\n\n\n\n\n1000 rows × 5 columns"
  },
  {
    "objectID": "08_anomaly.heuristics.html#heuristics",
    "href": "08_anomaly.heuristics.html#heuristics",
    "title": "Anomaly Detection Heuristics",
    "section": "Heuristics",
    "text": "Heuristics\nBest Threshold\n\nsource\n\nget_best_threshhold\n\n get_best_threshhold (th_df)\n\n\nbest_threshhold = get_best_threshhold(th_df)\nbest_threshhold\n\nLog Name          1001001\nF1 Score         0.953093\nPrecision        0.943888\nRecall           0.961224\nAnomaly Ratio       0.499\nName: 0.9430000000000001, dtype: object\n\n\nFixed Threshold\n\nsource\n\n\nget_fixed_heuristic\n\n get_fixed_heuristic (fixed, th_df)\n\n\nfixed = 0.98\n\nfixed_heuristic =get_fixed_heuristic(fixed,th_df)\nfixed_heuristic\n\nLog Name          1001001\nF1 Score         0.932661\nPrecision         0.97973\nRecall           0.887755\nAnomaly Ratio       0.444\nName: 0.98, dtype: object\n\n\nRatio Heuristic\n\nsource\n\n\nget_ratio_th\n\n get_ratio_th (ratio, th_df)\n\n\nratio =0.5\nratio_heuristic =get_ratio_th(ratio,th_df)\nratio_heuristic\n\nLog Name          1001001\nF1 Score         0.953093\nPrecision        0.943888\nRecall           0.961224\nAnomaly Ratio       0.499\nName: 0.9430000000000001, dtype: object\n\n\n\nhalf_ratio = partial(get_ratio_th,0.5)\nhalf_ratio(th_df)\n\nLog Name          1001001\nF1 Score         0.953093\nPrecision        0.943888\nRecall           0.961224\nAnomaly Ratio       0.499\nName: 0.9430000000000001, dtype: object\n\n\nEllbow Heuristic\n\nsource\n\n\nelbow_heuristic\n\n elbow_heuristic (th_df)\n\n\nellbow_down,ellbow_up = elbow_heuristic(th_df)\nellbow_down,ellbow_up\n\n(Log Name          1001001\n F1 Score         0.820048\n Precision        0.760188\n Recall           0.989796\n Anomaly Ratio       0.638\n Name: 0.6880000000000001, dtype: object,\n Log Name          1001001\n F1 Score          0.65711\n Precision        0.995851\n Recall           0.489796\n Anomaly Ratio       0.241\n Name: 0.998, dtype: object)\n\n\nLowest Plateau Heuristic\n\nsource\n\n\nget_lowest_plateau_heuristic\n\n get_lowest_plateau_heuristic (th_df)\n\n\nlp_min,lp_mean,lp_max=get_lowest_plateau_heuristic(th_df)\nlp_min,lp_mean,lp_max\n\n(Log Name          1001001\n F1 Score         0.941795\n Precision        0.977974\n Recall           0.906122\n Anomaly Ratio       0.454\n Name: 0.975, dtype: object,\n Log Name          1001001\n F1 Score         0.941795\n Precision        0.977974\n Recall           0.906122\n Anomaly Ratio       0.454\n Name: 0.975, dtype: object,\n Log Name          1001001\n F1 Score         0.941795\n Precision        0.977974\n Recall           0.906122\n Anomaly Ratio       0.454\n Name: 0.975, dtype: object)"
  },
  {
    "objectID": "08_anomaly.heuristics.html#visualization",
    "href": "08_anomaly.heuristics.html#visualization",
    "title": "Anomaly Detection Heuristics",
    "section": "Visualization",
    "text": "Visualization\n\nlog_name = '1001001'\nscore_df,y_true = get_score_df(log_name)\nths = np.array((range(100)))*0.001 +0.9\nth_df = get_th_df(ths,score_df,y_true,log_name)\nheuristics = [get_best_threshhold,partial(get_ratio_th,0.5),elbow_heuristic,\n              partial(get_fixed_heuristic,0.98),get_lowest_plateau_heuristic]\nheuristic_names = ['best','r_0.5','e_down','e_up','fix','lp_min','lp_mean','lp_max']\nheuristic_scores =[i for j in [listify(x(th_df)) for x in heuristics] for i in j]\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:00&lt;00:00]\n    \n    \n\n\n\n#export\ndef get_plot_data(th_df,id_vars=['Log Name'],value_vars = ['F1 Score','Anomaly Ratio']):\n    plot_data=th_df.melt(var_name='Metric',value_name='Score',id_vars =id_vars ,value_vars=value_vars,ignore_index=False)\n    plot_data['Threshold']=plot_data.index\n    plot_data.index=range(len(plot_data))\n    return plot_data\n\n\nplot_data = get_plot_data(th_df)\n\n\ncolor_palette = sns.color_palette()\nsns.set(rc={'figure.figsize':(18,10)})\n\nsns.lineplot(x=\"Threshold\", y=\"Score\",\n             hue=\"Metric\",\n             data=plot_data).set(title=f'Log: {log_name}')\nfor i in range(len(heuristic_scores)):\n    plt.axvline(heuristic_scores[i].name, 0,1,color=color_palette[i],linestyle='--',\n                label=f\"~{heuristic_names[i]}, {heuristic_scores[i].name}, {heuristic_scores[i]['F1 Score'].round(2)}\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\n\npd.DataFrame(zip([i.name for i in heuristic_scores],[i['F1 Score'] for i in heuristic_scores]),\n             columns=['Threshold','F1-Score'],index=heuristic_names)\n\n\n\n\n\n\n\n\nThreshold\nF1-Score\n\n\n\n\nbest\n0.943\n0.953093\n\n\nr_0.5\n0.943\n0.953093\n\n\ne_down\n0.994\n0.798393\n\n\ne_up\n0.998\n0.657110\n\n\nfix\n0.980\n0.932661\n\n\nlp_min\n0.980\n0.932661\n\n\nlp_mean\n0.980\n0.932661\n\n\nlp_max\n0.980\n0.932661"
  },
  {
    "objectID": "09_anomaly.performance_pdc.html",
    "href": "09_anomaly.performance_pdc.html",
    "title": "PDC Performance",
    "section": "",
    "text": "Includes the performance analysis for the PDC 2020 event logs.\n\n\nfrom fastai.basics import *\nfrom pympp.process import *\nfrom pympp.anomaly.detect import *\nfrom pympp.anomaly.heuristics import *\nfrom sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\nimport seaborn as sns\n\n\nget_preds??\n\n\ndef get_th_df(ths,score_df,y_true,log_name,f_score=f_score):\n    res = []\n    for t in ths:\n        y_pred=get_preds(score_df,t,)\n        anomaly_ratio = sum(x == 1 for x in y_pred) / len(y_pred)\n        f1 = f_score(y_true, y_pred)\n        precision = precision_score(y_true,y_pred)\n        recall = recall_score(y_true,y_pred)\n        res.append([log_name, f1, precision, recall, anomaly_ratio])\n\n    columns='Log Name', 'F1 Score','Precision','Recall', 'Anomaly Ratio'\n    th_df = pd.DataFrame(res,columns=columns,index=ths)\n    return th_df\n\n\nget_score_df??\n\n\npdc_year=2020\nheuristics = [get_best_threshhold,partial(get_ratio_th,0.5),elbow_heuristic,\n              partial(get_fixed_heuristic,0.98),get_lowest_plateau_heuristic]\nheuristic_names = ['best','r_0.5','e_down','e_up','fix','lp_min','lp_mean','lp_max']\nres = []\nfor fn in progress_bar(glob.glob(f'../data/logs/csv/dapnn_ds/PDC{pdc_year}_training/*')):\n    log_name = Path(fn).stem[9:-4]\n    score_df,y_true = get_score_df(log_name)\n    ths = np.array((range(100)))*0.001 +0.9\n    th_df = get_th_df(ths,score_df,y_true,log_name)\n\n    heuristic_scores_f1 =[i['F1 Score'] for j in [listify(x(th_df)) for x in heuristics] for i in j]\n    res.append([log_name,*heuristic_scores_f1])\n\npdc20_df =pd.DataFrame(res,columns=['Log']+heuristic_names)\npdc20_df\n\n\n\n\n\n\n    \n      \n      100.00% [192/192 03:04&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nLog\nbest\nr_0.5\ne_down\ne_up\nfix\nlp_min\nlp_mean\nlp_max\n\n\n\n\n0\n1001001\n0.953093\n0.953093\n0.798393\n0.657110\n0.932661\n0.932661\n0.932661\n0.932661\n\n\n1\n0111100\n0.931330\n0.927409\n0.905769\n0.911830\n0.924422\n0.928373\n0.928373\n0.928373\n\n\n2\n0000011\n0.937942\n0.935014\n0.924592\n0.589858\n0.898103\n0.898103\n0.898103\n0.898103\n\n\n3\n1000010\n0.961497\n0.952000\n0.894637\n0.874865\n0.960699\n0.949232\n0.949232\n0.949232\n\n\n4\n0201010\n0.812929\n0.812929\n0.806868\n0.792289\n0.692971\n0.709982\n0.709982\n0.711552\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n187\n1200101\n0.802969\n0.802969\n0.802969\n0.775232\n0.719553\n0.703018\n0.703018\n0.703018\n\n\n188\n0111001\n0.917743\n0.893800\n0.866523\n0.714097\n0.916810\n0.916000\n0.916000\n0.916000\n\n\n189\n0110111\n0.877301\n0.874412\n0.873815\n0.747505\n0.869146\n0.873815\n0.873815\n0.873815\n\n\n190\n1210101\n0.856760\n0.822154\n0.836382\n0.797863\n0.845096\n0.850585\n0.849805\n0.851043\n\n\n191\n1011111\n0.913332\n0.907029\n0.717697\n0.898628\n0.898628\n0.878180\n0.878180\n0.878180\n\n\n\n\n192 rows × 9 columns\n\n\n\n\npdc20_df=pdc20_df.sort_values(['best'])\n\n\nmeans=pdc20_df[list(pdc20_df)[1:]].mean()\nmean_f1_score=means[1]\nmeans\n\nbest       0.891134\nr_0.5      0.878399\ne_down     0.832037\ne_up       0.777214\nfix        0.861649\nlp_min     0.861107\nlp_mean    0.861229\nlp_max     0.861314\ndtype: float64\n\n\n\ndef get_plot_data(th_df,id_vars=['Log Name'],value_vars = ['F1 Score','Anomaly Ratio']):\n    plot_data=th_df.melt(var_name='Heuristic',value_name='Score',id_vars =id_vars ,value_vars=value_vars,ignore_index=False)\n    plot_data['Threshold']=plot_data.index\n    plot_data.index=range(len(plot_data))\n    return plot_data\n\nplot_data = get_plot_data(pdc20_df,id_vars=['Log'],value_vars=['best',\n 'e_down',\n 'fix',\n 'lp_mean',\n])\n\n\nlog2int = {j:i for i,j in enumerate(pdc20_df['Log'].unique())}\nplot_data ['Log by Index']= plot_data['Log'].map(log2int)\nplot_data ['F1 Score']=plot_data ['Score']\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nsns.scatterplot(data =plot_data, x='Log by Index',y='F1 Score', hue='Heuristic' ,linewidth=0,)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n&lt;matplotlib.legend.Legend&gt;"
  },
  {
    "objectID": "09_anomaly.performance_pdc.html#pdc-2020-performance-analysis",
    "href": "09_anomaly.performance_pdc.html#pdc-2020-performance-analysis",
    "title": "PDC Performance",
    "section": "",
    "text": "Includes the performance analysis for the PDC 2020 event logs.\n\n\nfrom fastai.basics import *\nfrom pympp.process import *\nfrom pympp.anomaly.detect import *\nfrom pympp.anomaly.heuristics import *\nfrom sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\nimport seaborn as sns\n\n\nget_preds??\n\n\ndef get_th_df(ths,score_df,y_true,log_name,f_score=f_score):\n    res = []\n    for t in ths:\n        y_pred=get_preds(score_df,t,)\n        anomaly_ratio = sum(x == 1 for x in y_pred) / len(y_pred)\n        f1 = f_score(y_true, y_pred)\n        precision = precision_score(y_true,y_pred)\n        recall = recall_score(y_true,y_pred)\n        res.append([log_name, f1, precision, recall, anomaly_ratio])\n\n    columns='Log Name', 'F1 Score','Precision','Recall', 'Anomaly Ratio'\n    th_df = pd.DataFrame(res,columns=columns,index=ths)\n    return th_df\n\n\nget_score_df??\n\n\npdc_year=2020\nheuristics = [get_best_threshhold,partial(get_ratio_th,0.5),elbow_heuristic,\n              partial(get_fixed_heuristic,0.98),get_lowest_plateau_heuristic]\nheuristic_names = ['best','r_0.5','e_down','e_up','fix','lp_min','lp_mean','lp_max']\nres = []\nfor fn in progress_bar(glob.glob(f'../data/logs/csv/dapnn_ds/PDC{pdc_year}_training/*')):\n    log_name = Path(fn).stem[9:-4]\n    score_df,y_true = get_score_df(log_name)\n    ths = np.array((range(100)))*0.001 +0.9\n    th_df = get_th_df(ths,score_df,y_true,log_name)\n\n    heuristic_scores_f1 =[i['F1 Score'] for j in [listify(x(th_df)) for x in heuristics] for i in j]\n    res.append([log_name,*heuristic_scores_f1])\n\npdc20_df =pd.DataFrame(res,columns=['Log']+heuristic_names)\npdc20_df\n\n\n\n\n\n\n    \n      \n      100.00% [192/192 03:04&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nLog\nbest\nr_0.5\ne_down\ne_up\nfix\nlp_min\nlp_mean\nlp_max\n\n\n\n\n0\n1001001\n0.953093\n0.953093\n0.798393\n0.657110\n0.932661\n0.932661\n0.932661\n0.932661\n\n\n1\n0111100\n0.931330\n0.927409\n0.905769\n0.911830\n0.924422\n0.928373\n0.928373\n0.928373\n\n\n2\n0000011\n0.937942\n0.935014\n0.924592\n0.589858\n0.898103\n0.898103\n0.898103\n0.898103\n\n\n3\n1000010\n0.961497\n0.952000\n0.894637\n0.874865\n0.960699\n0.949232\n0.949232\n0.949232\n\n\n4\n0201010\n0.812929\n0.812929\n0.806868\n0.792289\n0.692971\n0.709982\n0.709982\n0.711552\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n187\n1200101\n0.802969\n0.802969\n0.802969\n0.775232\n0.719553\n0.703018\n0.703018\n0.703018\n\n\n188\n0111001\n0.917743\n0.893800\n0.866523\n0.714097\n0.916810\n0.916000\n0.916000\n0.916000\n\n\n189\n0110111\n0.877301\n0.874412\n0.873815\n0.747505\n0.869146\n0.873815\n0.873815\n0.873815\n\n\n190\n1210101\n0.856760\n0.822154\n0.836382\n0.797863\n0.845096\n0.850585\n0.849805\n0.851043\n\n\n191\n1011111\n0.913332\n0.907029\n0.717697\n0.898628\n0.898628\n0.878180\n0.878180\n0.878180\n\n\n\n\n192 rows × 9 columns\n\n\n\n\npdc20_df=pdc20_df.sort_values(['best'])\n\n\nmeans=pdc20_df[list(pdc20_df)[1:]].mean()\nmean_f1_score=means[1]\nmeans\n\nbest       0.891134\nr_0.5      0.878399\ne_down     0.832037\ne_up       0.777214\nfix        0.861649\nlp_min     0.861107\nlp_mean    0.861229\nlp_max     0.861314\ndtype: float64\n\n\n\ndef get_plot_data(th_df,id_vars=['Log Name'],value_vars = ['F1 Score','Anomaly Ratio']):\n    plot_data=th_df.melt(var_name='Heuristic',value_name='Score',id_vars =id_vars ,value_vars=value_vars,ignore_index=False)\n    plot_data['Threshold']=plot_data.index\n    plot_data.index=range(len(plot_data))\n    return plot_data\n\nplot_data = get_plot_data(pdc20_df,id_vars=['Log'],value_vars=['best',\n 'e_down',\n 'fix',\n 'lp_mean',\n])\n\n\nlog2int = {j:i for i,j in enumerate(pdc20_df['Log'].unique())}\nplot_data ['Log by Index']= plot_data['Log'].map(log2int)\nplot_data ['F1 Score']=plot_data ['Score']\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nsns.scatterplot(data =plot_data, x='Log by Index',y='F1 Score', hue='Heuristic' ,linewidth=0,)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n&lt;matplotlib.legend.Legend&gt;"
  },
  {
    "objectID": "09_anomaly.performance_pdc.html#compare-with-existing-approaches",
    "href": "09_anomaly.performance_pdc.html#compare-with-existing-approaches",
    "title": "PDC Performance",
    "section": "Compare with existing approaches",
    "text": "Compare with existing approaches\nThis image is from the PDC 2020 website:\n\n\n\nImage of Yaktocat\n\n\nWe copy over the results from the existing approaches from the graphic and enter the score of our DAPNN approach:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n\ncols= ['Method','F1 Score']\n\ndata =[\n    ['Alpha Miner',0.03],\n    ['Directly Follows Miner',0.66],\n    ['Flower Miner',0.41],\n    ['Fodina Miner',0.68],\n    ['Hybrid ILP Miner',0.4],\n    ['Inductive Miner',0.33],\n    ['Log Skeletopn',0.73],\n    ['Log Skeleton (5% noise)',0.8],\n    ['Split Miner',0.17],\n    ['Trace Miner',0.34],\n    ['DAPNN Best',0.89],\n    ['DAPNN Fix-98',0.86],\n    ['DAPNN LP',0.86],\n    ['DAPNN Elbow↓',0.84],\n\n\n]\ndf=pd.DataFrame(data,columns=cols)\ndf['F-Score']=df['F1 Score']*100\n\n# add the plot\n\n\n\nax=sns.barplot(x='F-Score',y='Method',data=df)\n# add the annotation\nax.set_xlim(0,100)\nax.set_xticks(range(0,110,10))\nax.set_ylabel(\"\")\n\nText(0, 0.5, '')"
  },
  {
    "objectID": "09_anomaly.performance_pdc.html#pdc-2021-performance-analysis",
    "href": "09_anomaly.performance_pdc.html#pdc-2021-performance-analysis",
    "title": "PDC Performance",
    "section": "PDC 2021 Performance Analysis",
    "text": "PDC 2021 Performance Analysis\n\nIncludes the performance analysis for the PDC 2021 event logs.\n\n\nCompute\n\ndef get_score_df(log_name,prediction_normalization = True):\n    learner_path=f'../models/pdc2021'\n    training_log_path = f'../data/logs/csv/dapnn_ds/PDC2021_training/pdc2021_{log_name}.csv.gz'\n    test_log_path = f'../data/logs/csv/dapnn_ds/PDC2021_ground_truth/pdc2021_{log_name[:-1]}.csv.gz' \n    \n    cols = ['activity']\n    m, categorify= load_pred_model(learner_path,training_log_path,f\"pdc2021_{log_name}\")\n    if type(test_log_path)==str:\n        log = import_log_pdc(test_log_path)\n    else:\n        log = test_log_path   \n    o = process_test(log,categorify,cols)\n    nsp,idx=predict_next_step(o,m)\n    score_df=multivariate_anomaly_score(nsp,o,idx,cols,prediction_normalization)\n    y_true =o.items['normal'].astype(float).groupby(o.items.index).mean().to_numpy() ==False\n    return score_df, y_true\n\n\npdc_year=2021\nheuristics = [get_best_threshhold,partial(get_ratio_th,0.5),elbow_heuristic,\n              partial(get_fixed_heuristic,0.98),get_lowest_plateau_heuristic]\nheuristic_names = ['best','r_0.5','e_down','e_up','fix','lp_min','lp_mean','lp_max']\nres = []\nfor fn in progress_bar(glob.glob(f'../data/logs/csv/dapnn_ds/PDC{pdc_year}_training/*')):\n    log_name = Path(fn).stem[8:-4]\n    score_df,y_true = get_score_df(log_name)\n    ths = np.array((range(100)))*0.001 +0.9\n    th_df = get_th_df(ths,score_df,y_true,log_name)\n\n    heuristic_scores_f1 =[i['F1 Score'] for j in [listify(x(th_df)) for x in heuristics] for i in j]\n    res.append([log_name,*heuristic_scores_f1])\n\npdc21_df =pd.DataFrame(res,columns=['Log']+heuristic_names)\npdc21_df\n\n\n\n\n\n\n    \n      \n      100.00% [480/480 03:59&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nLog\nbest\nr_0.5\ne_down\ne_up\nfix\nlp_min\nlp_mean\nlp_max\n\n\n\n\n0\n0010011\n0.971193\n0.947983\n0.786408\n0.971193\n0.853211\n0.971193\n0.971193\n0.971193\n\n\n1\n0200003\n1.000000\n1.000000\n0.983740\n0.983740\n1.000000\n0.991935\n0.991935\n0.991935\n\n\n2\n1001114\n0.987854\n0.987854\n0.983740\n0.966942\n0.983740\n0.975410\n0.975410\n0.975410\n\n\n3\n1110100\n0.979853\n0.963983\n0.879279\n0.867709\n0.890214\n0.934906\n0.934906\n0.934906\n\n\n4\n0100113\n1.000000\n1.000000\n0.949580\n0.949580\n0.991935\n0.949580\n0.949580\n0.949580\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n475\n1001014\n0.995984\n0.995984\n0.995984\n0.975738\n0.995984\n0.991935\n0.991935\n0.991935\n\n\n476\n1100110\n0.971588\n0.971588\n0.963187\n0.971588\n0.963187\n0.966942\n0.966942\n0.966942\n\n\n477\n1000004\n1.000000\n1.000000\n0.983740\n0.966942\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n478\n0010101\n1.000000\n1.000000\n0.962656\n0.949580\n0.962656\n0.995984\n0.995984\n0.995984\n\n\n479\n1211112\n0.905022\n0.888000\n0.858493\n0.831694\n0.887351\n0.905022\n0.905022\n0.905022\n\n\n\n\n480 rows × 9 columns\n\n\n\n\nmeans=pdc21_df[list(pdc21_df)[1:]].mean()\nmean_f1_score=means[1]\nmeans\n\nbest       0.980649\nr_0.5      0.973581\ne_down     0.930354\ne_up       0.925034\nfix        0.949955\nlp_min     0.965082\nlp_mean    0.965082\nlp_max     0.965082\ndtype: float64\n\n\n\npdc21_df =pdc21_df.sort_values('best')\n\ndef get_plot_data(th_df,id_vars=['Log Name'],value_vars = ['F1 Score','Anomaly Ratio']):\n    plot_data=th_df.melt(var_name='Heuristic',value_name='Score',id_vars =id_vars ,value_vars=value_vars,ignore_index=False)\n    plot_data['Threshold']=plot_data.index\n    plot_data.index=range(len(plot_data))\n    return plot_data\n\nplot_data = get_plot_data(pdc21_df,id_vars=['Log'],value_vars=['best',\n 'e_down',\n 'fix',\n 'lp_mean',\n])\n\n\nlog2int = {j:i for i,j in enumerate(pdc21_df['Log'].unique())}\nplot_data ['Log by Index']= plot_data['Log'].map(log2int)\nplot_data ['F1 Score']=plot_data ['Score']\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nsns.scatterplot(data =plot_data, x='Log by Index',y='F1 Score', hue='Heuristic' ,linewidth=0,)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\n\n\nCompare with existing approaches\nThis image is from the PDC 2021 website:\n\n\n\nImage of Yaktocat\n\n\nWe copy over the results from the existing approaches from the graphic and enter the score of our DAPNN approach:\n\ncols= ['Method','F1 Score']\n\ndata =[\n    ['Alpha Miner',0.023],\n    ['Directly Follows ',0.89],\n    ['Directly Follows Model',0.719],\n        ['DisCoveR CW',0.958],\n    ['DisCoveR Light CW',0.953],\n    ['Flower Miner',0],\n    ['Fodina Miner',0.35],\n    ['Hybrid ILP',0.464],\n    ['Inductive IMfa',0.423],\n    ['Kokos 2 T5',0.349],\n    ['Log Skeleton N3',0.957],\n        ['Split Miner',0.258],\n\n    ['Trace Miner',0.154],\n\n    ['DAPNN Best',0.98],\n    ['DAPNN Fix-98',0.95],\n    ['DAPNN LP',0.97],\n    ['DAPNN Elbow↓',0.93],\n\n    \n]\ndf=pd.DataFrame(data,columns=cols)\ndf['F-Score']=df['F1 Score']*100\n\n# add the plot\n\n\n\nax=sns.barplot(x='F-Score',y='Method',data=df)\n# add the annotation\nax.set_xlim(0,105)\nax.set_xticks(range(0,110,10))\nax.set_ylabel(\"\")\n#ax.bar_label(ax.containers[-1], fmt='%.0f%%', label_type='edge');\n\nText(0, 0.5, '')"
  },
  {
    "objectID": "05_prediction.eval.html",
    "href": "05_prediction.eval.html",
    "title": "Prediction Evaluation",
    "section": "",
    "text": "_RUN_TRAINING=True"
  },
  {
    "objectID": "05_prediction.eval.html#runner",
    "href": "05_prediction.eval.html#runner",
    "title": "Prediction Evaluation",
    "section": "Runner",
    "text": "Runner\nThis section runs the process prediction experiment\n\nif _RUN_TRAINING:\n    res=runner(logs[0:1],ppms,attr_dict=attr_dict,sample=False,epoch=1,runs=1,store=True,bs=512,print_output=False)\n\n\n\n\n\n\n\n\n\n\nCPU times: user 32min 27s, sys: 4min 4s, total: 36min 32s\nWall time: 11min 49s"
  },
  {
    "objectID": "05_prediction.eval.html#shell-script",
    "href": "05_prediction.eval.html#shell-script",
    "title": "Prediction Evaluation",
    "section": "Shell script:",
    "text": "Shell script:\n\ndef isnotebook():\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == 'ZMQInteractiveShell':\n            return True   # Jupyter notebook or qtconsole\n        elif shell == 'TerminalInteractiveShell':\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False\n\ndef command_line(log_idx=range(len(logs)),ppm_idx=range(len(ppms)),sample=False,store=True, runs=1,\n                     bs=64,print_output=False,patience=3, min_delta=0.005, epoch=20, seed=42):\n    log_sel=L(logs)[log_idx]\n    ppm_sel=L(ppms)[ppm_idx]\n    runner(log_sel,ppm_sel,attr_dict=attr_dict, sample=sample,store=store,epoch=epoch,tqdm=tqdm_console,\n       print_output=print_output,bs=bs,patience=patience,min_delta=min_delta,runs=runs,seed=seed)\n\nif __name__ == '__main__' and not isnotebook():\n    fire.Fire(command_line)"
  },
  {
    "objectID": "prediction.predict.html",
    "href": "prediction.predict.html",
    "title": "Predict",
    "section": "",
    "text": "#hide\nbpi12 ='../data/logs/csv/mppn_ds/BPIC12.csv'\nlog=import_log(bpi12)\nlog\n\n\n\n\n\n\n\n\nevent_id\nresource\ntimestamp\nactivity\nREG_DATE\nAMOUNT_REQ\n\n\ntrace_id\n\n\n\n\n\n\n\n\n\n\n173688\n0\n112.0\n2011-09-30 22:38:44.546000+00:00\n###start###\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n1\n112.0\n2011-09-30 22:38:44.546000+00:00\nA_SUBMITTED_COMPLETE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n2\n112.0\n2011-09-30 22:38:44.880000+00:00\nA_PARTLYSUBMITTED_COMPLETE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n3\n112.0\n2011-09-30 22:39:37.906000+00:00\nA_PREACCEPTED_COMPLETE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n4\n112.0\n2011-09-30 22:39:38.875000+00:00\nW_Completeren aanvraag_SCHEDULE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n214376\n3\n112.0\n2012-02-29 22:52:01.287000+00:00\nW_Afhandelen leads_SCHEDULE\n2012-02-29 23:51:16.799000+01:00\n15000\n\n\n214376\n4\n11169.0\n2012-03-01 08:26:46.736000+00:00\nW_Afhandelen leads_START\n2012-02-29 23:51:16.799000+01:00\n15000\n\n\n214376\n5\n11169.0\n2012-03-01 08:27:37.118000+00:00\nA_DECLINED_COMPLETE\n2012-02-29 23:51:16.799000+01:00\n15000\n\n\n214376\n6\n11169.0\n2012-03-01 08:27:41.325000+00:00\nW_Afhandelen leads_COMPLETE\n2012-02-29 23:51:16.799000+01:00\n15000\n\n\n214376\n7\n11169.0\n2012-03-01 08:27:41.325000+00:00\n###end###\n2012-02-29 23:51:16.799000+01:00\n15000\n\n\n\n\n288374 rows × 6 columns"
  },
  {
    "objectID": "prediction.predict.html#sample-training",
    "href": "prediction.predict.html#sample-training",
    "title": "Predict",
    "section": "Sample Training",
    "text": "Sample Training\n\ncol='activity'\no=PPObj(log,procs=Categorify(),cat_names=col,y_names=col,splits=split_traces(log))\no.show()\n\n#traces: 12616 #events: 249149\n\n\n\n\n\n\nactivity\n\n\ntrace_id\n\n\n\n\n\n193068\n2\n\n\n193068\n12\n\n\n193068\n9\n\n\n\n\n\n\ndls=o.get_dls()\nxb,yb=dls.one_batch()\nxb.shape,yb[0].shape\n\n(torch.Size([64, 1, 5]), torch.Size([64]))\n\n\n\npath = \"../data/logs/csv/mppn_ds/Helpdesk.csv\"\n\n\nlog=import_log(path)\nlog.head()\n\n\n\n\n\n\n\n\nevent_id\ntimestamp\nactivity\nresource\n\n\ntrace_id\n\n\n\n\n\n\n\n\nCase1\n0\n2012-10-09 11:50:17+00:00\n###start###\nValue 1\n\n\nCase1\n1\n2012-10-09 11:50:17+00:00\nAssign seriousness\nValue 1\n\n\nCase1\n2\n2012-10-09 11:51:01+00:00\nTake in charge ticket\nValue 1\n\n\nCase1\n3\n2012-10-12 12:02:56+00:00\nTake in charge ticket\nValue 2\n\n\nCase1\n4\n2012-10-25 08:54:26+00:00\nResolve ticket\nValue 1\n\n\n\n\n\n\n\n\ncol='activity'\no=PPObj(log,procs=Categorify(),cat_names=col,y_names=col,splits=split_traces(log))\no.show()\n\n#traces: 4580 #events: 30508\n\n\n\n\n\n\nactivity\n\n\ntrace_id\n\n\n\n\n\nCase3756\n2\n\n\nCase3756\n3\n\n\nCase3756\n14\n\n\n\n\n\n\ndls=o.get_dls()\nxb,yb=dls.one_batch()\nxb.shape,yb[0].shape,xb[0], len(o.procs.categorify[col])\n\n(torch.Size([64, 1, 5]),\n torch.Size([64]),\n tensor([[ 0,  2,  3, 14, 16]], device='cuda:0'),\n 17)\n\n\n\nsource\n\nRNNwEmbedding\n\n RNNwEmbedding (o)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nm=RNNwEmbedding(o)\n\n\nsource\n\n\naccuracy_targ0\n\n accuracy_targ0 (inp, targ, axis=-1)\n\nCompute accuracy with targ when pred is bs * n_classes\n\nsource\n\n\ncross_entropy_targ0\n\n cross_entropy_targ0 (inp, targ)\n\n\nsource\n\n\nHideOutput\n\n HideOutput ()\n\nA utility function that hides all outputs in a context\n\np=m(xb.cpu())\n\n\nsource\n\n\ntraining_loop\n\n training_loop (learn, epoch, print_output, lr_find)\n\nBasic training loop that uses learning rate finder and one cycle training. See fastai docs for more information\n\nsource\n\n\ntrain_validate\n\n train_validate (dls, m, metrics=&lt;function accuracy&gt;, loss=&lt;function\n                 cross_entropy&gt;, epoch=20, print_output=True,\n                 model_dir='.', lr_find=True, output_index=1, patience=3,\n                 min_delta=0.005, show_plot=True, store_path='tmp',\n                 model_name='.model')\n\nTrains a model on the training set with early stopping based on the validation loss. Afterwards, applies it to the test set.\n\ncols,outcome='activity',False\no=PPObj(log,procs=Categorify(),cat_names=cols,y_names=cols,splits=split_traces(log))\ndls=o.get_dls(outcome=outcome,windows=partial(windows_fast))\nm=RNNwEmbedding(o)\ntrain_validate(dls,m,epoch=5,metrics=accuracy_targ0,loss=lambda x,y: F.cross_entropy(x,y[0]),print_output=False,show_plot=False)\n\n0.858601450920105"
  },
  {
    "objectID": "prediction.predict.html#process-prediction-model",
    "href": "prediction.predict.html#process-prediction-model",
    "title": "Predict",
    "section": "Process Prediction Model",
    "text": "Process Prediction Model\nThe PPModel class creates multiple prediction models for next-step prediction, next-resource prediction, remaining time prediction, etc. based on a pytorch model architecture.\n\nsource\n\nPPModel\n\n PPModel (log, ds_name, splits, store=None, bs=64, print_output=False,\n          patience=3, min_delta=0.005, attr_dict=None, windows=&lt;function\n          windows_fast&gt;, epoch=20, sample=False, train_validate=&lt;function\n          train_validate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nget_ds_name\n\n get_ds_name (url)\n\n\n\n\n\nDetails\n\n\n\n\nurl\nUtility function, that gets the name of a dataset\n\n\n\n\nsource\n\n\nPPM_RNNwEmbedding\n\n PPM_RNNwEmbedding (log, ds_name, splits, store=None, bs=64,\n                    print_output=False, patience=3, min_delta=0.005,\n                    attr_dict=None, windows=&lt;function windows_fast&gt;,\n                    epoch=20, sample=False, train_validate=&lt;function\n                    train_validate&gt;)\n\nSampe PPM based on RNNwEmbedding\n\nlog=import_log(path)\nds_name=get_ds_name(path)\nsplits=split_traces(log)\n\n\nppm=PPM_RNNwEmbedding(log,ds_name,splits,epoch=1)\n\n\nppm.next_step_prediction()\n\n0.853634774684906"
  },
  {
    "objectID": "prediction.predict.html#pipeline-runner",
    "href": "prediction.predict.html#pipeline-runner",
    "title": "Predict",
    "section": "Pipeline Runner",
    "text": "Pipeline Runner\nA runner function for PPModel. Runs multiple prediction models on various datasets for several runs. Writes the results to a dataframe and stores it on disk in the ./tmp folder. Stores the splits, the trained models, and the results. Accepts multiple parameters.\n\nsource\n\nPerformance_Statistic\n\n Performance_Statistic ()\n\nCreates a results dataframe, that shows the performance of all models on all datasets on all tasks.\n\nsource\n\n\nrunner\n\n runner (dataset_urls, ppm_classes, store=True, runs=1, sample=False,\n         seed=42, tqdm=&lt;class 'tqdm.notebook.tqdm_notebook'&gt;, bs=64,\n         print_output=False, patience=3, min_delta=0.005, attr_dict=None,\n         windows=&lt;function windows_fast&gt;, epoch=20,\n         train_validate=&lt;function train_validate&gt;)\n\nThis sample shows how to use the runner function:\n\ndataset_paths=[path]\nppms=[PPM_RNNwEmbedding,PPM_RNNwEmbedding]\nres=runner(dataset_paths,ppms,epoch=1,store=True,print_output=False,\n           sample=True,runs=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 2.92 s, sys: 23.1 s, total: 26 s\nWall time: 27.7 s\n\n\n\nprint (\"RUN\",0)\ndisplay_df(res[0])\n\nprint (\"RUN\",1)\ndisplay_df(res[1])\n\nRUN 0\nRUN 1\n\n\n\n\n\n\nDataset\nModel\nNext Step\nNext Resource\nLast Resource\nOutcome\nNext relative Timestamp\nDuration to Outcome\nActivity Suffix\nResource Suffix\n\n\n\n\n0\nHelpdesk\nRNNwEmbedding\n0.525424\n0.322034\n0.694915\n1.0\nNone\nNone\nNone\nNone\n\n\n1\nHelpdesk\nRNNwEmbedding\n0.635593\n0.254237\n0.694915\n1.0\nNone\nNone\nNone\nNone\n\n\n\n\n\n\n\n\n\nDataset\nModel\nNext Step\nNext Resource\nLast Resource\nOutcome\nNext relative Timestamp\nDuration to Outcome\nActivity Suffix\nResource Suffix\n\n\n\n\n0\nHelpdesk\nRNNwEmbedding\n0.313559\n0.262712\n0.694915\n1.0\nNone\nNone\nNone\nNone\n\n\n1\nHelpdesk\nRNNwEmbedding\n0.177966\n0.245763\n0.694915\n1.0\nNone\nNone\nNone\nNone\n\n\n\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ntorch.cuda.device_count()\n\n1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pympp",
    "section": "",
    "text": "git clone https://github.com/joLahann/pympp\ncd pympp\npip install ."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "pympp",
    "section": "",
    "text": "git clone https://github.com/joLahann/pympp\ncd pympp\npip install ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "pympp",
    "section": "How to use",
    "text": "How to use\nProcess Prediction\n\nfrom pympp.process import *\nfrom pympp.prediction.predict import *\n\n\nlog = import_log('./data/logs/csv/mppn_ds/BPIC12.csv')\nlog.head()\n\n\n\n\n\n\n\n\nevent_id\nresource\ntimestamp\nactivity\nREG_DATE\nAMOUNT_REQ\n\n\ntrace_id\n\n\n\n\n\n\n\n\n\n\n173688\n0\n112.0\n2011-09-30 22:38:44.546000+00:00\n###start###\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n1\n112.0\n2011-09-30 22:38:44.546000+00:00\nA_SUBMITTED_COMPLETE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n2\n112.0\n2011-09-30 22:38:44.880000+00:00\nA_PARTLYSUBMITTED_COMPLETE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n3\n112.0\n2011-09-30 22:39:37.906000+00:00\nA_PREACCEPTED_COMPLETE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n4\n112.0\n2011-09-30 22:39:38.875000+00:00\nW_Completeren aanvraag_SCHEDULE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n\n\n\n\n\n\ncol = 'activity'\no = PPObj(log,procs = Categorify(),cat_names=col,y_names=col,splits=split_traces(log))\no.show()\n\n#traces: 12616 #events: 249149\n\n\n\n\n\n\nactivity\n\n\ntrace_id\n\n\n\n\n\n214361\n2\n\n\n214361\n12\n\n\n214361\n9\n\n\n\n\n\n\ndls = o.get_dls()\nm = RNNwEmbedding(o)\ntrain_res = train_validate(dls,m,epoch=5,metrics=accuracy_targ0,loss=cross_entropy_targ0,print_output=False,show_plot=False)\n\n\ntrain_res\n\n0.8209230303764343\n\n\nAnomaly Detection\n\nfrom pympp.anomaly.detect import *\nfrom fastai.basics import *\nfrom sklearn.metrics import f1_score\n\n\nlog = import_log('./data/logs/csv/dapnn_ds/PDC2020_training/pdc_2020_0101100.csv.gz')\nlog.head()\n\n\n\n\n\n\n\n\nactivity\nevent_id\n\n\ntrace_id\n\n\n\n\n\n\ntrace 1\n###start###\n0\n\n\ntrace 1\nt31\n1\n\n\ntrace 1\nt41\n2\n\n\ntrace 1\nt26\n3\n\n\ntrace 1\nt36\n4\n\n\n\n\n\n\n\n\no,dls,categorify = training_dl(log)\n\n\nm=ControlFlowModel(o)\ntrain_res =  train_validate(dls,m,epoch=25,metrics=accuracy_targ0,loss=cross_entropy_targ0,print_output=False,show_plot=False)\ntrain_res\n\n(#2) [1.0479528903961182,0.5110132098197937]\n\n\n\ntest_log = import_log_pdc('../data/logs/csv/dapnn_ds/PDC2020_ground_truth/pdc_2020_0101100.csv.gz')\ntest_log.head()\n\n\n\n\n\n\n\n\nactivity\ncase:pdc:costs\nnormal\nevent_id\n\n\ntrace_id\n\n\n\n\n\n\n\n\ntrace 1\n###start###\n0.0\nTrue\n0\n\n\ntrace 1\nt21\n0.0\nTrue\n1\n\n\ntrace 1\nt32\n0.0\nTrue\n2\n\n\ntrace 1\nt26\n0.0\nTrue\n3\n\n\ntrace 1\nt41\n0.0\nTrue\n4\n\n\n\n\n\n\n\n\no = process_test(test_log,categorify)\nwds,idx=windows_fast(o.xs, o.event_ids)\nres=(m(LongTensor(wds).cuda()))\nanomaly_score = calc_anomaly_score(res,o,idx)\ny_pred, y_true = get_anomalies(anomaly_score,o,idx)\nf1_score(y_true, y_pred)\n\n0.8571428571428571\n\n\nBuild with nbdev and fastai."
  },
  {
    "objectID": "04_prediction.mppn.html",
    "href": "04_prediction.mppn.html",
    "title": "MPPN",
    "section": "",
    "text": "This notebook includes the implementation of the MPPN as described in the paper. It includes the pytorch models, a training procedure containing pre-processing and data loader generation, the implementation of the PPModel based on MPPN’s pytorch models for each process prediction task."
  },
  {
    "objectID": "04_prediction.mppn.html#mppn-models",
    "href": "04_prediction.mppn.html#mppn-models",
    "title": "MPPN",
    "section": "MPPN Models",
    "text": "MPPN Models\n\nsource\n\nBaseMPPN\n\n BaseMPPN (num_perspectives, feature_size=64, output_dim=128)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nMPPNClassifier\n\n MPPNClassifier (num_perspectives, num_classes, feature_size=64,\n                 output_dim=128, with_softmax=True)\n\nExtends Base MPPN with one classification layer.\n\nsource\n\n\nMPPNMultitask\n\n MPPNMultitask (num_perspectives, output_attr, feature_size=64,\n                representation_dim=128)\n\nExtends the MPPNClassifier with multiple heads to predict multiple outputs at once.\n\nsource\n\n\nload_checkpoint\n\n load_checkpoint (path, filename)\n\nLoad checkpoint from disk\n\n\n\n\nType\nDetails\n\n\n\n\npath\n\n\n\n\nfilename\n\n\n\n\nReturns\n\n\n\n\n\n\nsource\n\n\nmppn_pretraining_model\n\n mppn_pretraining_model (pretrained=False, **kwargs)\n\nReturns a model either pretrained as alexnet or on GAF images.\n\nsource\n\n\nmppn_representation_learning_model\n\n mppn_representation_learning_model (pretrained, num_perspectives,\n                                     output_attr, feature_size=64,\n                                     representation_dim=128)\n\nReturns a model for representation learning (multitask). CNN is pretrained on GAF images\n\nsource\n\n\nmppn_fine_tuning_model\n\n mppn_fine_tuning_model (representation_model, num_perspectives,\n                         num_classes)\n\nFine-tune a MPPN model that has been trained as representation model on a specific task\n\n\n\n\nType\nDetails\n\n\n\n\nrepresentation_model\n\n\n\n\nnum_perspectives\n\n\n\n\nnum_classes\n\n\n\n\nReturns"
  },
  {
    "objectID": "04_prediction.mppn.html#mppn-training",
    "href": "04_prediction.mppn.html#mppn-training",
    "title": "MPPN",
    "section": "MPPN Training",
    "text": "MPPN Training\n\nsource\n\ngaf_transform\n\n gaf_transform (gs=64)\n\nGAF transormation: converts a bash into a gramian angular field and reshapes it to RGB\n\nv=torch.rand((50,10,64)),10\ngt=gaf_transform()\n\n\nx,y=gt(v)\n\nCPU times: user 3.65 s, sys: 1.52 s, total: 5.16 s\nWall time: 4.9 s\n\n\n\nsource\n\n\nmppn_get_output_attributes\n\n mppn_get_output_attributes (o)\n\nUtility function that puts vocab size of each output attribute in a dict. For regression tasks, it adds vocab size 1\nThe next few cells show how to create a dataloader for MPPN and apply them to the representation learning and the finetuning step:\n\n# Create mppn repreentation learning and create the dataloader\npath=\"../data/logs/csv/mppn_ds/Mobis.csv\"\nlog=import_log(path)\n_t=attr_dict[get_ds_name(path)]\ncat_names,cont_names,date_names=[_t[i] for i in list(_t)]\no=PPObj(log,[Categorify,Datetify,FillMissing,MinMax],\n        cat_names=cat_names,cont_names=cont_names,date_names=date_names,\n        splits=split_traces(log),y_names=['activity','resource','timestamp_Relative_elapsed_minmax'])\noutput_attributes=mppn_get_output_attributes(o)\nm = mppn_representation_learning_model(False, len(o.cont_names), output_attributes)\ndls=o.get_dls(after_batch=gaf_transform,bs=64,windows= partial(windows_fast,ws=64))\n\nLoading Alexnet to train MPPNs CNN from scratch\n\n\n\n# Get one batch\nxb,yb=dls.one_batch()\n\n\n# Get loss and metrics\nloss=partial(multi_loss_sum,o)\nmetrics=get_metrics(o)\n\n\n# Test forward pass\nxb=xb.cpu()\np=m(xb)\nloss(p,[y.cpu() for y in yb])\n\nTensorBase(3648850., grad_fn=&lt;AliasBackward&gt;)\n\n\n\n# Train representation learning model\nif _RUN_TRAINING:\n    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,show_plot=False)\n\n\ndef maeDurDaysNormalize(p,yb,mean=0,std=0,unit=60*60*24):\n    \"\"\"\n    Decodes time and converts from seconds to days\n    Returns mae\n    \"\"\"\n    p=p*std+mean\n    yb=yb*std+mean\n    return mae(p,yb)/(unit)"
  },
  {
    "objectID": "04_prediction.mppn.html#ppm",
    "href": "04_prediction.mppn.html#ppm",
    "title": "MPPN",
    "section": "PPM",
    "text": "PPM\nCreates the PPM for the MPPM. In the setup der general representation learning model is trained. Afterwards, in each prediction task, a seperate head is created, which is than fine-tuned for the specific task.\n\nsource\n\nPPM_MPPN\n\n PPM_MPPN (log, ds_name, splits, store=None, bs=64, print_output=False,\n           patience=3, min_delta=0.005, attr_dict=None, windows=&lt;function\n           windows_fast&gt;, epoch=20, sample=False, train_validate=&lt;function\n           train_validate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n_RUN_TRAINING=True\n\n\nif  _RUN_TRAINING:\n    path=\"../data/logs/csv/mppn_ds/BPIC12_O.csv\"\n    log=import_log(path)\n    ppm=PPM_MPPN(log,get_ds_name(path),split_traces(log),print_output=True,epoch=1,bs=512,attr_dict=attr_dict)\n    ppm.setup()\n\nLoading Alexnet to train MPPNs CNN from scratch\nBetter model found at epoch 0 with valid_loss value: 401655.5.\nBetter model found at epoch 0 with valid_loss value: 0.19436775147914886.\nCPU times: user 5min 18s, sys: 11.6 s, total: 5min 30s\nWall time: 1min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nact_acc\nact_res\nmaeDurDaysNormalize\ntime\n\n\n\n\n0\n503601.875000\n401655.500000\n0.194171\n0.042486\n4174838.750000\n00:06\n\n\n\n\n\n\n\n\n\n\n\n\n\nif _RUN_TRAINING: ppm.next_step_prediction()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\ntime\n\n\n\n\n0\n2067.743896\n2.293367\n0.092346\n00:06\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 2.2933671474456787.\nBetter model found at epoch 0 with valid_loss value: 0.09414687752723694.\nCPU times: user 5min 33s, sys: 11.9 s, total: 5min 45s\nWall time: 1min 1s\n\n\n\n\n\n\n\n\n\n\nif _RUN_TRAINING: ppm.duration_to_next_event_prediction()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmaeDurDaysNormalize\ntime\n\n\n\n\n0\n2597.336914\n0.824496\n8.591735\n00:06\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.8244962096214294.\nBetter model found at epoch 0 with valid_loss value: 8.738816261291504.\nCPU times: user 5min 30s, sys: 10.9 s, total: 5min 41s\nWall time: 1min\n\n\n\n\n\n\n\n\n\n\nif _RUN_TRAINING: ppm.duration_to_end_prediction()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmaeDurDaysNormalize\ntime\n\n\n\n\n0\n2026.624634\n2.063680\n21.504757\n00:06\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 2.0636796951293945.\nBetter model found at epoch 0 with valid_loss value: 21.64349365234375.\nCPU times: user 5min 32s, sys: 11.8 s, total: 5min 43s\nWall time: 1min 1s"
  },
  {
    "objectID": "03_prediction.models.html",
    "href": "03_prediction.models.html",
    "title": "Baseline implementations",
    "section": "",
    "text": "This notebook introduces a few loss and metric functions. Afterwards, eight models based on four papers are re-implemented in pytorch. For each model, a PPModel is created."
  },
  {
    "objectID": "03_prediction.models.html#loss-and-metrics",
    "href": "03_prediction.models.html#loss-and-metrics",
    "title": "Baseline implementations",
    "section": "Loss and Metrics",
    "text": "Loss and Metrics\nThis section defines some metrics and loss functions. Apart from that, we use the standart loss functions and metrics from fastai and pytorch, namely accuracy, mae, and cross_entropy\n\nsource\n\nmaeDurDaysNormalize\n\n maeDurDaysNormalize (p, yb, mean=0, std=0, unit=86400)\n\nDecodes time and converts from seconds to days Returns mae\n\nsource\n\n\nmaeDurDaysMinMax\n\n maeDurDaysMinMax (p, yb, minn=0, maxx=0, unit=86400)\n\nDecodes time and converts from seconds to days Returns mae\n\nsource\n\n\nAvgMetric\n\n AvgMetric (func, name)\n\nAverage the values of func taking into account potential different batch sizes\n\ndef accuracy(inp, targ, axis=-1):\n    pred,targ = flatten_check(inp.argmax(dim=axis), targ)\n    return (pred == targ).float().mean()\n\n\nsource\n\n\nget_metrics\n\n get_metrics (o, date_col='timestamp_Relative_elapsed')\n\nA utility function that automatically selects the correct metric functions based on the PPObj o\n\nsource\n\n\nmulti_loss_sum\n\n multi_loss_sum (o, p, y)\n\nMulti Loss function that sums up multiple loss functions. The selection of the loss function is based on the PPObj o"
  },
  {
    "objectID": "03_prediction.models.html#camargo",
    "href": "03_prediction.models.html#camargo",
    "title": "Baseline implementations",
    "section": "Camargo",
    "text": "Camargo\nInput: activity, resource, duration\nOutput: activity, resource, duration\nLoss: sum(cross_entropy(activity),cross_entropy(resource),mae(duration))\n\npath= '../data/logs/csv/mppn_ds/Helpdesk.csv'\n\n\nlog=import_log(path)\no=PPObj(log,[Categorify,Datetify,Normalize()],date_names=['timestamp'],cat_names=['activity','resource'],\n    y_names=['activity','resource','timestamp_Relative_elapsed'],splits=split_traces(log))\no\ndls=o.get_dls()\n\n\nxcat,xcont,y=dls.one_batch()\n\n\nxcat.shape,xcont.shape,len(y)\n\n(torch.Size([64, 2, 5]), torch.Size([64, 1, 5]), 3)\n\n\n\nSpezialized\n\nsource\n\n\nCamargo_specialized\n\n Camargo_specialized (o)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nm=Camargo_specialized(o)\n\n\np=m(xcat.cpu(),xcont.cpu())\n\n\nloss=partial(multi_loss_sum,o)\n\n\naccuracy??\n\n\nloss(p,tuple(i.cpu()for i in y))\n\nTensorBase(6.8803, grad_fn=&lt;AliasBackward&gt;)\n\n\n\nif _RUN_TRAINING:\n    metrics=get_metrics(o)\n    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,print_output=True,show_plot=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nacc_resource\nmae_days\ntime\n\n\n\n\n0\n5.087773\n5.077742\n0.845785\n0.501916\n5.414929\n00:06\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 5.077742099761963.\nBetter model found at epoch 0 with valid_loss value: 0.8500967025756836.\nCPU times: user 14.4 s, sys: 4.45 s, total: 18.9 s\nWall time: 19.6 s\n\n\n\n\n\n\n\n\n\n\n\nConcat\n\nsource\n\n\nCamargo_concat\n\n Camargo_concat (o)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nif _RUN_TRAINING:\n\n    log=import_log(path)\n    o=PPObj(log,[Categorify,Datetify,Normalize()],date_names=['timestamp'],cat_names=['activity','resource'],\n        y_names=['activity','resource','timestamp_Relative_elapsed'],splits=split_traces(log),)\n    dls=o.get_dls()\n    m=Camargo_concat(o)\n    loss=0\n    loss=partial(multi_loss_sum,o)\n    train_validate(dls,m,loss=loss,metrics=get_metrics(o),epoch=1,print_output=True,show_plot=False,output_index=[1,2,3])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nacc_resource\nmae_days\ntime\n\n\n\n\n0\n5.043581\n5.004405\n0.862868\n0.521487\n5.494439\n00:06\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 5.0044050216674805.\nBetter model found at epoch 0 with valid_loss value: 0.8609335422515869.\nCPU times: user 13.6 s, sys: 4.56 s, total: 18.1 s\nWall time: 18.7 s\n\n\n\n\n\n\n\n\n\n\n\nFull_Concat\n\nsource\n\n\nCamargo_fullconcat\n\n Camargo_fullconcat (o)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nif  _RUN_TRAINING:\n    log=import_log(path)\n    o=PPObj(log,[Categorify,Datetify,Normalize],date_names=['timestamp'],cat_names=['activity','resource'],\n        y_names=['activity','resource','timestamp_Relative_elapsed'],splits=split_traces(log),)\n    dls=o.get_dls()\n    m=Camargo_fullconcat(o)\n    loss=0\n    loss=partial(multi_loss_sum,o)\n    train_validate(dls,m,loss=loss,metrics=get_metrics(o),epoch=1,print_output=True,output_index=[1,2,3])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nacc_resource\nmae_days\ntime\n\n\n\n\n0\n4.997516\n4.959108\n0.854572\n0.517996\n3.629107\n00:05\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 4.959108352661133.\nBetter model found at epoch 0 with valid_loss value: 0.8306697010993958.\nCPU times: user 12.6 s, sys: 4.56 s, total: 17.1 s\nWall time: 17.6 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPPMS\n\nsource\n\n\nPPM_Camargo_Spezialized\n\n PPM_Camargo_Spezialized (log, ds_name, splits, store=None, bs=64,\n                          print_output=False, patience=3, min_delta=0.005,\n                          attr_dict=None, windows=&lt;function windows_fast&gt;,\n                          epoch=20, sample=False, train_validate=&lt;function\n                          train_validate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPPM_Camargo_fullconcat\n\n PPM_Camargo_fullconcat (log, ds_name, splits, store=None, bs=64,\n                         print_output=False, patience=3, min_delta=0.005,\n                         attr_dict=None, windows=&lt;function windows_fast&gt;,\n                         epoch=20, sample=False, train_validate=&lt;function\n                         train_validate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPPM_Camargo_concat\n\n PPM_Camargo_concat (log, ds_name, splits, store=None, bs=64,\n                     print_output=False, patience=3, min_delta=0.005,\n                     attr_dict=None, windows=&lt;function windows_fast&gt;,\n                     epoch=20, sample=False, train_validate=&lt;function\n                     train_validate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nif _RUN_TRAINING:\n    log=import_log(path)\n    ppm=PPM_Camargo_fullconcat(log,get_ds_name(path),print_output=True,epoch=1,bs=512,\n                               splits=split_traces(log))\n    ppm.evaluate()\n\nHelpdesk Camargo_fullconcat\nNext event prediction training\nBetter model found at epoch 0 with valid_loss value: 5.782820701599121.\nBetter model found at epoch 0 with valid_loss value: 0.37526652216911316.\nLast event prediction training\nBetter model found at epoch 0 with valid_loss value: 4.761327743530273.\nBetter model found at epoch 0 with valid_loss value: 1.0.\nnext_step_prediction\nnext_resource_prediction\nlast_resource_prediction\noutcome_prediction\nduration_to_next_event_prediction\nduration_to_end_prediction\nactivity_suffix_prediction\nresource_suffix_prediction\nCPU times: user 18.4 s, sys: 19.9 s, total: 38.2 s\nWall time: 40.7 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nacc_resource\nmae_days\ntime\n\n\n\n\n0\n6.144738\n5.782821\n0.373098\n0.366716\n7.118637\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nacc_resource\nmae_days\ntime\n\n\n\n\n0\n5.491952\n4.761328\n1.000000\n0.786942\n7.440363\n00:01"
  },
  {
    "objectID": "03_prediction.models.html#evermann",
    "href": "03_prediction.models.html#evermann",
    "title": "Baseline implementations",
    "section": "Evermann",
    "text": "Evermann\nInput: activity or resource\nOutput: activity or resource\nLoss: cross_entropy(activity) or cross_entropy(resource)\n\nsource\n\nEvermann\n\n Evermann (o)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\ncol='activity'\no=PPObj(log,procs=[Categorify],cat_names=col,y_names=col,splits=split_traces(log))\ndls=o.get_dls()\nxb,yb=dls.one_batch()\n\n\nm=Evermann(o)\n\n\npb=m(xb.cpu())\nF.cross_entropy(pb,yb[0].cpu()),accuracy(pb,yb[0].cpu())\n\n(tensor(2.8337, grad_fn=&lt;NllLossBackward&gt;), TensorBase(0.))\n\n\n\nif _RUN_TRAINING:\n    loss=partial(multi_loss_sum,o)\n    metrics=get_metrics(o)\n    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,show_plot=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\ntime\n\n\n\n\n0\n2.076243\n2.067339\n0.863592\n00:03\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 2.067338705062866.\nBetter model found at epoch 0 with valid_loss value: 0.8511332869529724.\nCPU times: user 6.37 s, sys: 4.57 s, total: 10.9 s\nWall time: 11.5 s\n\n\n\n\n\n\n\n\n\n\n\nPPM\n\nsource\n\n\nPPM_Evermann\n\n PPM_Evermann (log, ds_name, splits, store=None, bs=64,\n               print_output=False, patience=3, min_delta=0.005,\n               attr_dict=None, windows=&lt;function windows_fast&gt;, epoch=20,\n               sample=False, train_validate=&lt;function train_validate&gt;)\n\nSampe PPM based on RNNwEmbedding\n\nif _RUN_TRAINING:\n    runner([path],[PPM_Evermann],\n           bs=1024,epoch=1,print_output=False)\n\n\n\n\n\n\n\n\n\n\nCPU times: user 21 s, sys: 1min 10s, total: 1min 31s\nWall time: 1min 36s"
  },
  {
    "objectID": "03_prediction.models.html#tax",
    "href": "03_prediction.models.html#tax",
    "title": "Baseline implementations",
    "section": "Tax",
    "text": "Tax\nInput: activity or duration\nOutput: activity or duration\nLoss: cross_entropy(activity) or mae(duration) in days\n\nsource\n\nTax_et_al_spezialized\n\n Tax_et_al_spezialized (o)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nlog=import_log(path)\n\n\ndatetify=Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed'])\no=PPObj(log,[Categorify,OneHot,datetify,Normalize],cat_names='activity',splits=split_traces(log),\n        date_names='timestamp',y_names=['activity','timestamp_Relative_elapsed'])\ndls=o.get_dls()\n\n\nxcat,xcont,yb=(dls.one_batch())\n\n\no.cont_names\n\n(#3) ['timestamp_secSinceSunNoon','timestamp_secSinceNoon','timestamp_Relative_elapsed']\n\n\n\nm=Tax_et_al_spezialized(o)\n\n\np=m(xcat.cpu(),xcont.cpu())\n\n\nloss=partial(multi_loss_sum,o)\n\n\nif _RUN_TRAINING:\n    datetify=Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed'])\n    o=PPObj(log,[Categorify,OneHot,datetify,Normalize],cat_names='activity',splits=split_traces(log),\n            date_names='timestamp',y_names=['activity','timestamp_Relative_elapsed'])\n    dls=o.get_dls()\n    m=Tax_et_al_spezialized(o)\n    loss=partial(multi_loss_sum,o)\n    metrics=get_metrics(o)\n    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,show_plot=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nmae_days\ntime\n\n\n\n\n0\n2.600010\n2.525728\n0.691973\n5.334630\n00:05\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 2.525728464126587.\nBetter model found at epoch 0 with valid_loss value: 0.6947409510612488.\n\n\n\n\n\n\n\n\n\n\n\nShared\n\nsource\n\n\nTax_et_al_shared\n\n Tax_et_al_shared (o)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nm=Tax_et_al_shared(o)\n\n\np=m(xcat.cpu(),xcont.cpu())\n\n\nif _RUN_TRAINING:\n    datetify=Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed'])\n    o=PPObj(log,[Categorify,OneHot,datetify,Normalize],cat_names='activity',splits=split_traces(log),\n            date_names='timestamp',y_names=['activity','timestamp_Relative_elapsed'])\n    m=Tax_et_al_shared(o)\n    dls=o.get_dls()\n    loss=partial(multi_loss_sum,o)\n    metrics=get_metrics(o)\n    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,show_plot=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nmae_days\ntime\n\n\n\n\n0\n2.301209\n2.290444\n0.834453\n3.649318\n00:04\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 2.2904436588287354.\nBetter model found at epoch 0 with valid_loss value: 0.8342846035957336.\n\n\n\n\n\n\n\n\n\n\n\nMixed\n\nsource\n\n\nTax_et_al_mixed\n\n Tax_et_al_mixed (o, numlayers_shared=3, numlayers_single=3)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nm=Tax_et_al_mixed(o)\n\n\np=m(xcat.cpu(),xcont.cpu())\n\n\nlog=import_log('../data/logs/csv/mppn_ds/BPIC12.csv')\ntraces=split_traces(log)[0][:100]\nsplits=traces[:60],traces[60:80],traces[80:100]\nsplits=split_traces(log)\n\n\nif _RUN_TRAINING:\n    datetify=Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed'])\n    o=PPObj(log,[Categorify,OneHot,datetify,Normalize],cat_names='activity',splits=splits,\n            date_names='timestamp',y_names=['activity','timestamp_Relative_elapsed'])\n    m=Tax_et_al_mixed(o)  \n    dls=o.get_dls()\n    loss=partial(multi_loss_sum,o)\n    metrics=get_metrics(o)\n    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,output_index=[1,2],lr_find=False)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nmae_days\ntime\n\n\n\n\n0\n3.696305\n3.682654\n0.080829\n0.484208\n00:55\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 3.68265438079834.\nBetter model found at epoch 0 with valid_loss value: 0.08318328112363815.\n\n\n\n\n\n\n\n\n\n\n\nPPM\n\nsource\n\n\nPPM_Tax_Spezialized\n\n PPM_Tax_Spezialized (log, ds_name, splits, store=None, bs=64,\n                      print_output=False, patience=3, min_delta=0.005,\n                      attr_dict=None, windows=&lt;function windows_fast&gt;,\n                      epoch=20, sample=False, train_validate=&lt;function\n                      train_validate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPPM_Tax_Mixed\n\n PPM_Tax_Mixed (log, ds_name, splits, store=None, bs=64,\n                print_output=False, patience=3, min_delta=0.005,\n                attr_dict=None, windows=&lt;function windows_fast&gt;, epoch=20,\n                sample=False, train_validate=&lt;function train_validate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPPM_Tax_Shared\n\n PPM_Tax_Shared (log, ds_name, splits, store=None, bs=64,\n                 print_output=False, patience=3, min_delta=0.005,\n                 attr_dict=None, windows=&lt;function windows_fast&gt;,\n                 epoch=20, sample=False, train_validate=&lt;function\n                 train_validate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nif _RUN_TRAINING:\n    log=import_log(path)\n    ppm=PPM_Tax_Mixed(log,get_ds_name(path),sample=True,print_output=True,epoch=1,bs=512,splits=split_traces(log))\n    ppm.evaluate()\n\nHelpdesk Tax_Mixed\nNext event prediction training\nBetter model found at epoch 0 with valid_loss value: 3.148423433303833.\nBetter model found at epoch 0 with valid_loss value: 0.2049180269241333.\nLast event prediction training\nBetter model found at epoch 0 with valid_loss value: 1.9037986993789673.\nBetter model found at epoch 0 with valid_loss value: 1.0.\nnext_step_prediction\nnext_resource_prediction\nlast_resource_prediction\noutcome_prediction\nduration_to_next_event_prediction\nduration_to_end_prediction\nactivity_suffix_prediction\nresource_suffix_prediction\nCPU times: user 681 ms, sys: 3.56 s, total: 4.24 s\nWall time: 4.55 s\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nmae_days\ntime\n\n\n\n\n0\n3.285685\n3.148423\n0.222222\n16.522135\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\nmae_days\ntime\n\n\n\n\n0\n2.826721\n1.903799\n1.000000\n7.031667\n00:01"
  },
  {
    "objectID": "03_prediction.models.html#mida",
    "href": "03_prediction.models.html#mida",
    "title": "Baseline implementations",
    "section": "Mida",
    "text": "Mida\nInput: multi categorical atts and multi cont atts\nOutput: activity or resource or duration\nLoss: cross_entropy(activity) or cross_entropy(resource) or mae(duration) in days\n\nlog=import_log(path)\n\n\no=PPObj(log,[Categorify,Normalize,Datetify,FillMissing],\n        cat_names=['activity','resource'],date_names=['timestamp'],y_names='activity',splits=split_traces(log))\ndls=o.get_dls()\n\n\nxcat,xcont,yb=dls.one_batch()\n\n\nsource\n\nMiDA\n\n MiDA (o, seq_len=64)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nm=MiDA(o)\n\n\np=m(xcat.cpu(),xcont.cpu())\n\n\nif  _RUN_TRAINING:\n    log=import_log(path)\n    o=PPObj(log,[Categorify,Normalize,Datetify,FillMissing],\n            cat_names=['activity','resource'],date_names=['timestamp'],\n            splits=split_traces(log))\n    o.set_y_names('timestamp_Relative_elapsed')\n    dls=o.get_dls(bs=512)\n    seq_len=o.items.event_id.max()\n    m=MiDA(o,seq_len)\n    loss=partial(multi_loss_sum,o)\n    metrics=get_metrics(o)\n    train_validate(dls,m,epoch=5,loss=loss,metrics=metrics,print_output=True,show_plot=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmae_days\ntime\n\n\n\n\n0\n0.650651\n0.392814\n7.372935\n00:01\n\n\n1\n0.383053\n0.234714\n4.405476\n00:01\n\n\n2\n0.286920\n0.208739\n3.917929\n00:01\n\n\n3\n0.242524\n0.204839\n3.844725\n00:01\n\n\n4\n0.217379\n0.202689\n3.804385\n00:01\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.392814040184021.\nBetter model found at epoch 1 with valid_loss value: 0.23471425473690033.\nBetter model found at epoch 2 with valid_loss value: 0.2087388038635254.\nBetter model found at epoch 3 with valid_loss value: 0.20483869314193726.\nBetter model found at epoch 4 with valid_loss value: 0.20268948376178741.\nBetter model found at epoch 0 with valid_loss value: 3.545297145843506.\nCPU times: user 8.54 s, sys: 16 s, total: 24.5 s\nWall time: 26.4 s\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nPPM_MiDA\n\n PPM_MiDA (log, ds_name, splits, store=None, bs=64, print_output=False,\n           patience=3, min_delta=0.005, attr_dict=None, windows=&lt;function\n           windows_fast&gt;, epoch=20, sample=False, train_validate=&lt;function\n           train_validate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\ncreate_attr_dict\n\n create_attr_dict (attr_list)\n\n\nif _RUN_TRAINING:\n    log=import_log(path)\n    ppm=PPM_MiDA(log,get_ds_name(path),print_output=True,epoch=1,bs=512,attr_dict=attr_dict,splits=split_traces(log))\n    ppm.setup()\n    ppm.next_step_prediction()\n\n['activity']\nBetter model found at epoch 0 with valid_loss value: 2.7015204429626465.\nBetter model found at epoch 0 with valid_loss value: 0.19725391268730164.\nCPU times: user 6.37 s, sys: 11 s, total: 17.3 s\nWall time: 18.8 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc_activity\ntime\n\n\n\n\n0\n2.744843\n2.701520\n0.198484\n00:01"
  },
  {
    "objectID": "07_anomaly.training.html",
    "href": "07_anomaly.training.html",
    "title": "Training",
    "section": "",
    "text": "Training of Prediction Models for Anomaly Detection"
  },
  {
    "objectID": "07_anomaly.training.html#training-procedure",
    "href": "07_anomaly.training.html#training-procedure",
    "title": "Training",
    "section": "Training Procedure",
    "text": "Training Procedure\n\nsource\n\ntrain\n\n train (fn, log_name, store_path='models', epoch=25, ws=5)"
  },
  {
    "objectID": "07_anomaly.training.html#pdc-2020",
    "href": "07_anomaly.training.html#pdc-2020",
    "title": "Training",
    "section": "PDC 2020",
    "text": "PDC 2020\n\nsource\n\ntrain_pdc20_logs\n\n train_pdc20_logs (epoch=25)\n\n\n# train_pdc20_logs(epoch=1)"
  },
  {
    "objectID": "07_anomaly.training.html#pdc-2021",
    "href": "07_anomaly.training.html#pdc-2021",
    "title": "Training",
    "section": "PDC 2021",
    "text": "PDC 2021\n\nsource\n\ntrain_pdc21_logs\n\n train_pdc21_logs (epoch=25, rel_path='./')\n\n\n# train_pdc21_logs(epoch=1)"
  },
  {
    "objectID": "07_anomaly.training.html#binet-datasets",
    "href": "07_anomaly.training.html#binet-datasets",
    "title": "Training",
    "section": "Binet datasets",
    "text": "Binet datasets\n\nsource\n\ntrain_binet_logs\n\n train_binet_logs (epoch=25)\n\n\n# train_binet_logs(epoch=1)"
  },
  {
    "objectID": "07_anomaly.training.html#shell-utils",
    "href": "07_anomaly.training.html#shell-utils",
    "title": "Training",
    "section": "Shell Utils",
    "text": "Shell Utils\n\nsource\n\nrun_training\n\n run_training (log='all', epoch=25)\n\n\n# Runs the complete Training - takes arround 3 days\n\n\n\n\n\n\n    \n      \n      0.00% [0/192 00:00&lt;?]\n    \n    \n\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "01_process.html",
    "href": "01_process.html",
    "title": "Process",
    "section": "",
    "text": "Fill in a module description here"
  },
  {
    "objectID": "01_process.html#import-log",
    "href": "01_process.html#import-log",
    "title": "Process",
    "section": "Import Log",
    "text": "Import Log\nColumn renaming and adding of start and end events\n\nsource\n\ndf_preproc\n\n df_preproc (df, cols=['activity'], start_marker='###start###',\n             end_marker='###end###')\n\nAdd event_id column, as start and end tokens for each trace - Copys entire row. - Adds special start and end marker for col in cols\nImports an event log in csv format\n- rename activity and trace_id column\n\nsource\n\n\nimport_log\n\n import_log (log_path, cols=['activity'])\n\nImports an event log in csv format - rename activity and trace_id column - rename anomaly column\n\nlog_path = './data/logs/csv/mppn_ds/BPIC12.csv'\nlog = import_log(log_path)\nlog.head()\n\n\n\n\n\n\n\n\nevent_id\nresource\ntimestamp\nactivity\nREG_DATE\nAMOUNT_REQ\n\n\ntrace_id\n\n\n\n\n\n\n\n\n\n\n173688\n0\n112.0\n2011-09-30 22:38:44.546000+00:00\n###start###\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n1\n112.0\n2011-09-30 22:38:44.546000+00:00\nA_SUBMITTED_COMPLETE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n2\n112.0\n2011-09-30 22:38:44.880000+00:00\nA_PARTLYSUBMITTED_COMPLETE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n3\n112.0\n2011-09-30 22:39:37.906000+00:00\nA_PREACCEPTED_COMPLETE\n2011-10-01 00:38:44.546000+02:00\n20000\n\n\n173688\n4\n112.0\n2011-09-30 22:39:38.875000+00:00\nW_Completeren aanvraag_SCHEDULE\n2011-10-01 00:38:44.546000+02:00\n20000"
  },
  {
    "objectID": "01_process.html#trace-splitting",
    "href": "01_process.html#trace-splitting",
    "title": "Process",
    "section": "Trace Splitting",
    "text": "Trace Splitting\ni.e. splitting in training, validation and test set\nThe split_traces function is used to split an event_log into training, validation and test set. Furthermore, it removes traces that are longer than a specific threshhold.\n\nsource\n\ndrop_long_traces\n\n drop_long_traces (df, max_trace_len=64, event_id='event_id')\n\nDrop traces longer than max_trace_len\n\nsource\n\n\nRandomTraceSplitter\n\n RandomTraceSplitter (split_pct=0.2, seed=None)\n\nCreate function that splits items between train/val with valid_pct randomly.\n\nsource\n\n\nsplit_traces\n\n split_traces (df, df_name='tmp', seed=None)\n\nSplit traces in training, validation and test sets\n\na1,b1,c1=split_traces(log)\na2,b2,c2=split_traces(log)\ntest_ne(a1,a2), test_ne(b1,b2), test_ne(c1,c2);\n\n\nseed = 42\na1,b1,c1=split_traces(log,seed = seed)\na2,b2,c2=split_traces(log,seed = seed)\ntest_eq(a1,a2), test_eq(b1,b2), test_eq(c1,c2);"
  },
  {
    "objectID": "01_process.html#ppobj",
    "href": "01_process.html#ppobj",
    "title": "Process",
    "section": "PPObj",
    "text": "PPObj\nManages the pre-processing and provides utility functiosn for date columns, cat columns and cont columns.\n\nsource\n\nPPObj\n\n PPObj (df, procs=None, cat_names=None, cont_names=None, date_names=None,\n        y_names=None, splits=None, ycat_names=None, ycont_names=None,\n        inplace=False, do_setup=True)\n\nMain Class for Process Prediction\n\nppObj=PPObj(log,cat_names=['activity'],y_names=['activity'])\nppObj.show()\n\n#traces: 13087 #events: 288374\n\n\n\n\n\n\nactivity\n\n\ntrace_id\n\n\n\n\n\n173688\n###start###\n\n\n173688\nA_SUBMITTED_COMPLETE\n\n\n173688\nA_PARTLYSUBMITTED_COMPLETE\n\n\n\n\n\nWe can define various pre-processing functions that are executed, when PPOBj is instantiated. PPProc is the base class for a pre-processing function. It ensures, that setup of a pre-processing function is performed using the training set, and than it is applied to the validation and test set, with the same parameters.\n\nsource\n\n\nPPProc\n\n PPProc (enc=None, dec=None, split_idx=None, order=None)\n\nBase class to write a non-lazy tabular processor for dataframes\n\n\nCategorization\ni.e ordinal encoding\nImplementation of ordinal or integer encoding. Adds NA values for unknown data. Implementation is pretty much taken from fastai.\n\nsource\n\n\nCategorify\n\n Categorify (enc=None, dec=None, split_idx=None, order=None)\n\nTransform the categorical variables to something similar to pd.Categorical\n\nlog=import_log('../data/logs/csv/dapnn_ds/PDC2021_ground_truth/pdc2021_000000.csv.gz')\ntraces=split_traces(log)[0][:100]\nsplits=traces[:60],traces[60:80],traces[80:100]\no=PPObj(log,None,cat_names='activity',splits=splits)\n\n\nm=CategoryMap(o.items.loc[:,'activity'])\nlen(m)\n\n50\n\n\n\ncat=Categorify()\ncat.setup(o)\nlen(cat['activity'])\n\n51\n\n\n\ndf = pd.DataFrame({'a':[0,1,2,0,2]})\nto = PPObj(df, Categorify, 'a')\nto.show()\n\n#traces: 5 #events: 5\n\n\n\n\n\n\na\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n\n\n\n\nlog=import_log('./data/logs/csv/dapnn_ds/binet_logs/bpic12-0.3-1.csv.gz')\no=PPObj(log,Categorify,'activity')\no.show()\n\n#traces: 13087 #events: 289892\n\n\n\n\n\n\nactivity\n\n\ntrace_id\n\n\n\n\n\n173688\n2\n\n\n173688\n12\n\n\n173688\n9\n\n\n\n\n\n\n\nFill Missing\nfor continuous values\nA pre-processing function that deals with missing data in continuous attributes. Missing data can be replaced with the median, mean or a constant value. Additionaly, we can create another boolean column that indicates, which rows were missing. Implementation is pretty much taken from fastai.\n\nsource\n\n\nFillStrategy\n\n FillStrategy ()\n\nNamespace containing the various filling strategies.\n\nsource\n\n\nFillMissing\n\n FillMissing (fill_strategy=&lt;function median&gt;, add_col=True,\n              fill_vals=None)\n\nBase class to write a non-lazy tabular processor for dataframes\n\nfill = FillMissing() \ndf = pd.DataFrame({'a':[0,1,np.nan,1,2,3,4], 'b': [0,1,2,3,4,5,6]})\nto = PPObj(df, fill, cont_names=['a', 'b'])\nto.show()\n\n#traces: 7 #events: 7\n\n\n\n\n\n\na_na\na\nb\n\n\n\n\n0\nFalse\n0.0\n0\n\n\n1\nFalse\n1.0\n1\n\n\n2\nTrue\n1.5\n2\n\n\n\n\n\n\n\nZ-score\n\nsource\n\n\nNormalize\n\n Normalize (enc=None, dec=None, split_idx=None, order=None)\n\nNormalize with z-score\n\ndf = pd.DataFrame({'a':[0,1,9,3,4]})\nto = PPObj(df, Normalize(), cont_names='a')\nto.show()\n\n#traces: 5 #events: 5\n\n\n\n\n\n\na\n\n\n\n\n0\n-1.429409\n\n\n1\n-1.327783\n\n\n2\n-0.514775\n\n\n\n\n\n\n\nDate conversion\nEncodes a date column. Supports multiple information by using pandas date functions. This implementation is also based on the fastai but also supports relative duration from the first event of a case.\n\ndf = pd.DataFrame({'fu': ['2019-12-04', '2019-11-29', '2019-11-15', '2019-10-24']})\n_make_date(df, 'fu')\ndf.dtypes\n\nfu    datetime64[ns, UTC]\ndtype: object\n\n\n\nsource\n\n\nencode_date\n\n encode_date (df, field_name, unit=1000000000.0, date_encodings=['Year',\n              'Month', 'Day', 'Dayofweek', 'Dayofyear', 'Elapsed'])\n\nHelper function that adds columns relevant to a date in the column field_name of df.\n\ndf = pd.DataFrame({'fu': ['2019-12-04', '2019-11-29', '2019-11-15', '2019-10-24']})\nencode_date(df,'fu')\ndf\n\n\n\n\n\n\n\n\nfu_Year\nfu_Month\nfu_Day\nfu_Dayofweek\nfu_Dayofyear\nfu_Elapsed\n\n\n\n\n0\n2019\n12\n4\n2\n338\n1.575418e+09\n\n\n1\n2019\n11\n29\n4\n333\n1.574986e+09\n\n\n2\n2019\n11\n15\n4\n319\n1.573776e+09\n\n\n3\n2019\n10\n24\n3\n297\n1.571875e+09\n\n\n\n\n\n\n\n\nsource\n\n\ndecode_date\n\n decode_date (df, field_name, unit=1000000000.0, date_encodings=['Year',\n              'Month', 'Day', 'Dayofweek', 'Dayofyear', 'Elapsed'])\n\n\ndecode_date(df,'fu')\ndf\n\n\n\n\n\n\n\n\nfu\n\n\n\n\n0\n2019-12-04 00:00:00+00:00\n\n\n1\n2019-11-29 00:00:00+00:00\n\n\n2\n2019-11-15 00:00:00+00:00\n\n\n3\n2019-10-24 00:00:00+00:00\n\n\n\n\n\n\n\n\nsource\n\n\nDatetify\n\n Datetify (date_encodings=['Relative_elapsed'])\n\nEncode dates,\n\ndf = pd.DataFrame({'fu': ['2019-10-04', '2019-10-09', '2019-10-15', '2019-10-24']},index=[1,1,1,1])\no = PPObj(df,Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed']),date_names='fu')\no.xs\n\n\n\n\n\n\n\n\nfu_secSinceSunNoon\nfu_secSinceNoon\nfu_Relative_elapsed\n\n\n\n\n1\n259200\n0\n0.0\n\n\n1\n86400\n0\n432000.0\n\n\n1\n0\n0\n950400.0\n\n\n1\n172800\n0\n1728000.0\n\n\n\n\n\n\n\n\ndf = pd.DataFrame({'fu': ['2019-10-04', '2019-10-09', '2019-10-15', '2019-10-24']},index=[1,1,1,1])\no = PPObj(df,[Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed']),Normalize],date_names='fu')\no.xs\n\n\n\n\n\n\n\n\nfu_secSinceSunNoon\nfu_secSinceNoon\nfu_Relative_elapsed\n\n\n\n\n1\n-1.341627\n0.0\n-1.208083\n\n\n1\n-1.341645\n0.0\n-1.208082\n\n\n1\n-1.341655\n0.0\n-1.208080\n\n\n1\n-1.341636\n0.0\n-1.208079\n\n\n\n\n\n\n\n\no.procs.means\n\n{'fu_secSinceSunNoon': 129600.0,\n 'fu_secSinceNoon': 0.0,\n 'fu_Relative_elapsed': 777600.0}\n\n\n\n\nMinMax Scaling\nCalculates the MinMax scaling from a column.\n\nsource\n\n\nMinMax\n\n MinMax (enc=None, dec=None, split_idx=None, order=None)\n\nBase class to write a non-lazy tabular processor for dataframes\n\npath=\"../data/logs/csv/mppn_ds/BPIC12_O.csv\"\nlog=import_log(path)\ncat_names,cont_names,date_names=['activity', 'resource'], ['AMOUNT_REQ'], ['timestamp']\no=PPObj(log,[Categorify,Datetify,FillMissing,MinMax],\n                     cat_names=cat_names,date_names=date_names,cont_names=cont_names,\n                     y_names=['activity','resource','timestamp_Relative_elapsed'])\n\n\no.xs.dtypes\n\nactivity_minmax                      float64\nresource_minmax                      float64\nAMOUNT_REQ_minmax                    float64\ntimestamp_Relative_elapsed_minmax    float64\ndtype: object\n\n\n\n\nOne HoT Encoding\nCalculates the one-hot encoding of a column. It is required to first apply categorization on the same column, to deal with missing values.\n\nsource\n\n\nOneHot\n\n OneHot (enc=None, dec=None, split_idx=None, order=None)\n\nTransform the categorical variables to one-hot. Requires Categorify to deal with unseen data.\n\nevent_df=import_log('./data/logs/csv/dapnn_ds/PDC2021_training/pdc2021_0000000.csv.gz')\n\n\no=PPObj(event_df,[Categorify(),OneHot()],cat_names=['activity'])\n\nCPU times: user 26 ms, sys: 2.4 ms, total: 28.4 ms\nWall time: 28 ms"
  },
  {
    "objectID": "01_process.html#window-generation",
    "href": "01_process.html#window-generation",
    "title": "Process",
    "section": "Window Generation",
    "text": "Window Generation\nHere, we cover the sliding window generation\n\nsource\n\nwindows_fast\n\n windows_fast (df, event_ids, ws=5, pad=None)\n\n\nevent_df=import_log('../data/logs/csv/dapnn_ds/PDC2020_ground_truth/pdc_2020_0000000.csv.gz')\n\n\no=PPObj(event_df,Categorify(),cat_names=['activity'],y_names='activity')\n#o=o.iloc[0]\nlen(o)\n\n1000\n\n\n\nws,idx=windows_fast(o.xs,o.event_ids,ws=5)\nws,ws.shape\n\n(array([[[ 0,  0,  0,  0,  2]],\n \n        [[ 0,  0,  0,  2,  3]],\n \n        [[ 0,  0,  2,  3,  4]],\n \n        ...,\n \n        [[12, 15, 18, 19, 21]],\n \n        [[15, 18, 19, 21, 20]],\n \n        [[18, 19, 21, 20, 22]]], dtype=int8),\n (17029, 1, 5))"
  },
  {
    "objectID": "01_process.html#data-loader",
    "href": "01_process.html#data-loader",
    "title": "Process",
    "section": "Data Loader",
    "text": "Data Loader\nThe prefixes are converted to a pytorch.Dataset and than to a DataLoader A batch is than represented as a tuple of the form (x cat. attr,x cont. attr, y cat. attr., y cont attr.). Also, categorical attributes are converted to a long tensor and continous attributes to a float tensor.\nIf a dimensions of the batch is empty - e.g. the model does not use categorical input attributes - it is removed from the tuple.\n\no=PPObj(event_df,Categorify(),cat_names=['activity'],y_names='activity')\nws,idx=windows_fast(o.xs,o.event_ids,ws=10)\nws,ws.shape\n\n(array([[[ 0,  0,  0, ...,  0,  0,  2]],\n \n        [[ 0,  0,  0, ...,  0,  2,  3]],\n \n        [[ 0,  0,  0, ...,  2,  3,  4]],\n \n        ...,\n \n        [[11, 13,  6, ..., 18, 19, 21]],\n \n        [[13,  6, 10, ..., 19, 21, 20]],\n \n        [[ 6, 10, 16, ..., 21, 20, 22]]], dtype=int8),\n (17029, 1, 10))\n\n\n\nsource\n\nPPDset\n\n PPDset (inp)\n\nsome doc\n\n\nIntegration Samples\nThis section shows, how the PPObj can be used to create a DataLoader for pedictive process analytics:\nNext event prediction:\nX: ‘activity’\nY: ‘activity’\n\nlog=import_log('./data/logs/csv/dapnn_ds/PDC2020_ground_truth/pdc_2020_0000001.csv.gz')\no=PPObj(log,Categorify(),cat_names=['activity'],y_names='activity',splits=split_traces(event_df))\no\n\n           activity case:pdc:costs case:pdc:isPos event_id\ntrace_id                                                  \ntrace 336         2            2.0          False        0\ntrace 336         3            2.0          False        1\ntrace 336         5            2.0          False        2\ntrace 336         9            2.0          False        3\ntrace 336         4            2.0          False        4\n...             ...            ...            ...      ...\ntrace 186        18            7.0          False       13\ntrace 186        21            7.0          False       14\ntrace 186        20            7.0          False       15\ntrace 186        22            7.0          False       16\ntrace 186         1            7.0          False       17\n\n[18029 rows x 4 columns]\n\n\n\ndls=o.get_dls(windows=partial(windows_fast,ws=2))\nxb,y=dls.one_batch()\nxb.shape,y[0].shape\n\n(torch.Size([64, 1, 2]), torch.Size([64]))"
  }
]